{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77729750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0116a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device, threshold = 0.5):\n",
    "    # if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "    #     raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "    n_count = 0\n",
    "    criterion.to(device) #? Move criterion to device\n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            n_count += inputs.size(0) #? Count number of samples trained\n",
    "                                    \n",
    "            # print(all(param.requires_grad for param in model.parameters()))\n",
    "   \n",
    "            torch.set_printoptions(threshold=float('inf'))\n",
    "            assert not torch.isnan(inputs).any(), \"Input has NaNs\"\n",
    "            assert not torch.isinf(inputs).any(), \"Input has Infs\"\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            assert not torch.isnan(outputs).any(), \"Model output has NaNs\"\n",
    "            assert not torch.isinf(outputs).any(), \"Model output has Infs\"\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            assert not torch.isnan(loss).any(), \"Model loss has NaNs\"\n",
    "            loss.backward() #? Backpropagation\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            # scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                # labels = labels.cpu() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            loss_ratio = val_loss / train_loss    \n",
    "            pos_weight = loss_ratio  # or any other function of loss_ratio you choose\n",
    "            # scheduler.step(val_loss)  # Use validation loss here, not training loss\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update criterion with new pos_weight\n",
    "        # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss and epoch > 100:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        elif avg_val_loss*0.96 <= best_val_loss:\n",
    "                wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}\".ljust(12), \n",
    "              f\"training loss:{train_loss:.3f}\".ljust(16), \n",
    "              f\"best_val_loss:{best_val_loss:.3f}\".ljust(12), \n",
    "              f\"Val Loss: {avg_val_loss:.3f}\",\n",
    "              f\"Scheduler lr: {scheduler.get_last_lr()}\".ljust(50), \n",
    "              f\"N samples trained: {n_count}\",\n",
    "              end=\"\\r\")\n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                labels = labels.cpu() \n",
    "                # predictions = (torch.sigmoid(outputs) < 0.5).float().cpu().numpy()\n",
    "                predictions = (torch.sigmoid(outputs) >= threshold).float().cpu().numpy()\n",
    "                \n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy=accuracy_score(labels, predictions) \n",
    "    precision=precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
    "    recall=recall_score(labels, predictions, pos_label=1)\n",
    "    f1=f1_score(labels, predictions, pos_label=1)\n",
    "    auc=roc_auc_score(labels, predictions)\n",
    "    \n",
    "    return model, accuracy, precision, recall, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training_Helper_Functions import *\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch import optim\n",
    "def maximise_combined_score(trial):\n",
    "    if True:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", device)\n",
    "        epochs = 800\n",
    "        random_state = 42\n",
    "        # Scaler\n",
    "        scaler = RobustScaler() # Or StandardScaler() - You can also make this a hyperparameter if you want\n",
    "        threshold = trial.suggest_float(\"threshold\", 0.28,0.6,)\n",
    "        # dropout = None\n",
    "        initial_lr = trial.suggest_float(\"initial_lr\", 1e-3, 1e-3 ,log=True)\n",
    "        max_lr = trial.suggest_float(\"max_lr\", 1e-3, 1e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 6e-1, log=True)\n",
    "        # Loss function hyperparameters\n",
    "        criterion_choice = \"BCEWithLogitsLoss\" #trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\", \"FocalLoss\"])\n",
    "        # Hyperparameter exploration optimization\n",
    "        if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "            pos_weight = trial.suggest_int(\"pos_weight\",3, 3)\n",
    "            alpha = None\n",
    "            gamma = None\n",
    "        elif criterion_choice == \"FocalLoss\":\n",
    "            pos_weight = None\n",
    "            alpha = trial.suggest_float(\"alpha\", 0.25, 0.75)\n",
    "            gamma = trial.suggest_float(\"gamma\", 1.0, 5.0)\n",
    "        else:\n",
    "            pos_weight = None\n",
    "\n",
    "        # Initialize lists for metrics across folds\n",
    "        accuracy_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        auc_list = []\n",
    "    if True:\n",
    "        training_set = pd.read_csv('./DATA/folds/train_fold_0.csv')\n",
    "        cv_set = pd.read_csv('./DATA/folds/test_fold_0.csv')\n",
    "        holdout_set = pd.read_csv('./DATA/holdout_set/holdout_data_OHE.csv')\n",
    "        # print(training_set.shape)\n",
    "        # print(cv_set.shape)\n",
    "        # print(holdout_set.shape)\n",
    "\n",
    "        train_x = training_set.drop(columns = 'DR')\n",
    "        train_y = training_set[['DR']]\n",
    "        test_x = cv_set.drop(columns = 'DR')\n",
    "        test_y = cv_set[['DR']] \n",
    "    # Cross-validation loop\n",
    "    # for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "    train_x, test_x, train_y, test_y\n",
    "    fold = 1\n",
    "    print(f\"Fold {fold}:\")\n",
    "    # Create DataLoader for current fold\n",
    "    train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=512,\n",
    "                                                        device=device)\n",
    "    # Instantiate and initialize the model\n",
    "    model = Ivan_NN(input_dim=get_feature_count(train_loader))\n",
    "    model.to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # Map the choice to the actual loss function\n",
    "    criterion = criterion_mapping(criterion_choice, pos_weight, alpha, gamma)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay) \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "        optimiser,\n",
    "        base_lr=1e-6,\n",
    "        max_lr=max_lr,\n",
    "        cycle_momentum=True)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.1)\n",
    "\n",
    "    # Train and evaluate the model on the current fold\n",
    "    model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "        model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=epochs, patience=40,\n",
    "        device=device, threshold=threshold\n",
    "    )\n",
    "    print(f\"Accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, auc: {auc:.4f}\")\n",
    "    del model\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "\n",
    "    # Append the metrics from the current fold\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    # combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "    combined_score = avg_f1\n",
    "\n",
    "    return combined_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
