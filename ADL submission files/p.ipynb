{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fbeb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "from dataset.data_preprocessing import *\n",
    "from Preprocessing_Functions2 import *\n",
    "from Training_Helper_Functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_preprocess_data(\"dataset/raw_data.csv\") #? convert raw data to OHE and encoded version\n",
    "data = pd.read_csv(\"dataset/processed_data_encoded.csv\")\n",
    "\n",
    "#? Split training and holdout/test set\n",
    "trainVal, testing = train_test_split(data, test_size=0.1, random_state=42, stratify=data[\"DR\"])\n",
    "\n",
    "#? Split training and validation set\n",
    "training, validation = train_test_split(trainVal, test_size=1/9, random_state=42, stratify=trainVal[\"DR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad298e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CV_generator(training:pd.DataFrame, validation:pd.DataFrame, OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler()):\n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', \n",
    "             'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    X = training.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "    validation = validation.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "\n",
    "    #* OUTLIER DETECTION\n",
    "    X_train_processed = Outlier_Removal(X, \n",
    "                                    OD_majority=OD_majority,\n",
    "                                    OD_minority=OD_minority,\n",
    "    )\n",
    "    # #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "    print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "    #* Smote but we don't use it for training\n",
    "    # _train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "    X_train_processed = Synthetic_Data_Generator2(X_train_processed, \"\", synthesizer=synthesizer, epochs=epochs, batch_size=256, n_synthetic_data=n_synthetic_data)\n",
    "    print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "    \n",
    "    #* Calculate BMI, TCTG & ENCODING\n",
    "    X_train_processed, validation = get_bmi(X_train_processed, validation)\n",
    "    X_train_processed, validation = get_TCTG(X_train_processed, validation)\n",
    "    X_train_processed, validation = apply_one_hot_encoding(X_train_processed, validation)\n",
    "    #* Scaler\n",
    "    X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "    validation[cont_cols] = scaler.transform(validation[cont_cols])\n",
    "    # Append processed data (excluding the target column 'DR')\n",
    "\n",
    "    # Save to CSV with fold number\n",
    "    X_train_processed.to_csv(f\"./DATA/training80.csv\", index=False)\n",
    "    validation.to_csv(f\"./DATA/val10.csv\", index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "def final_generator(training:pd.DataFrame, test:pd.DataFrame,\n",
    "OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler()):\n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', \n",
    "             'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    X = training.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "    test = test.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "\n",
    "    #* OUTLIER DETECTION\n",
    "    X_train_processed = Outlier_Removal(X, \n",
    "                                    OD_majority=OD_majority,\n",
    "                                    OD_minority=OD_minority,\n",
    "    )\n",
    "    # #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "    print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "    #* Smote but we don't use it for training\n",
    "    # _train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "    X_train_processed = Synthetic_Data_Generator2(X_train_processed, \"\", synthesizer=synthesizer, epochs=epochs, batch_size=256, n_synthetic_data=n_synthetic_data)\n",
    "    print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "    \n",
    "    #* Calculate BMI, TCTG & ENCODING\n",
    "    X_train_processed, test = get_bmi(X_train_processed, test)\n",
    "    X_train_processed, test = get_TCTG(X_train_processed, test)\n",
    "    X_train_processed, test = apply_one_hot_encoding(X_train_processed, test)\n",
    "    #* Scaler\n",
    "    X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "    test[cont_cols] = scaler.transform(test[cont_cols])\n",
    "    # Append processed data (excluding the target column 'DR')\n",
    "\n",
    "    # Save to CSV with fold number\n",
    "    X_train_processed.to_csv(f\"./DATA/training90.csv\", index=False)\n",
    "    test.to_csv(f\"./DATA/test10.csv\", index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3537158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# CV_generator(training, validation, OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler())\n",
    "\n",
    "CV_generator(training, validation,\n",
    "            OD_majority = IQRDetector(factor=1.2),\n",
    "            OD_minority = IQRDetector(factor=1.8),\n",
    "            synthesizer = \"TVAE\",\n",
    "            epochs = 1000,\n",
    "            n_synthetic_data = 10000,\n",
    "            scaler=StandardScaler())\n",
    "\n",
    "# final_generator(training, testing, OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler())\n",
    "\n",
    "final_generator(training, testing,\n",
    "                OD_majority = IQRDetector(factor=1.2),\n",
    "                OD_minority = IQRDetector(factor=1.8),\n",
    "                synthesizer = \"TVAE\",\n",
    "                epochs = 1000,\n",
    "                n_synthetic_data = 10000,\n",
    "                scaler=StandardScaler())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b59258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training80 = pd.read_csv(\"./DATA/training80.csv\")\n",
    "val10 = pd.read_csv(\"./DATA/val10.csv\")\n",
    "training90 = pd.read_csv(\"./DATA/training90.csv\")\n",
    "test10 = pd.read_csv(\"./DATA/test10.csv\")\n",
    "\n",
    "training80_x = training80.drop(columns=[\"DR\"])\n",
    "training80_y = training80[[\"DR\"]]\n",
    "val10_x = val10.drop(columns=[\"DR\"]) \n",
    "val10_y = val10[[\"DR\"]]\n",
    "\n",
    "training90_x = training90.drop(columns=[\"DR\"])\n",
    "training90_y = training90[[\"DR\"]]\n",
    "test10_x = test10.drop(columns=[\"DR\"]) \n",
    "test10_y = test10[[\"DR\"]]\n",
    "\n",
    "train80_loader, val10_loader = fold_to_dataloader_tensor(training80_x, val10_x, training80_y, val10_y, batch_size=256)\n",
    "\n",
    "training90_loader, test10_loader = fold_to_dataloader_tensor(training90_x, test10_x, training90_y, test10_y, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd3dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Ivan_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, 2180)\n",
    "        self.input_bn = nn.BatchNorm1d(2180)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(2180, 888),\n",
    "            nn.BatchNorm1d(888),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(888, 1122),\n",
    "            nn.BatchNorm1d(1122),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(1122, 624),\n",
    "            nn.BatchNorm1d(624),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(624, 1080),\n",
    "            nn.BatchNorm1d(1080),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(1080, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),  # â† Fixed this\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Linear(128, 36),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Linear(36, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),  # or LeakyReLU, up to you\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Skip connection projectors\n",
    "        self.skip1_proj = nn.Sequential(nn.Linear(2180, 1122))\n",
    "        self.skip2_proj = nn.Sequential(nn.Linear(1122, 128))\n",
    "        self.skip3_proj = nn.Sequential(nn.Linear(128, 64))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.input_bn(x)\n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "\n",
    "        skip1 = self.skip1_proj(x)\n",
    "        x2 = x2 + skip1\n",
    "\n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        x5 = self.block5(x4)\n",
    "\n",
    "        x6 = self.block6(x5)\n",
    "\n",
    "        skip2 = self.skip2_proj(x2)\n",
    "        x6 = x6 + skip2\n",
    "\n",
    "        x7 = self.block7(x6)\n",
    "        x8 = self.block8(x7)\n",
    "\n",
    "        skip3 = self.skip3_proj(x6)\n",
    "        x8 = x8 + skip3\n",
    "\n",
    "        out = self.output(x8)\n",
    "        return out\n",
    "\n",
    "def train_model(model:Ivan_NN, nFeatures:int, train80_loader, val10_loader, epochs=10, learningRate = 0.0001, threshold:float = 0.5, patience = 30, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    theMODEL = model(nFeatures)\n",
    "    theMODEL.to(device)\n",
    "    print(theMODEL)\n",
    "\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(theMODEL.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies= []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_f1s = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_f1 = .0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        theMODEL.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in train80_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = theMODEL(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        theMODEL.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val10_loader:\n",
    "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "                outputs = theMODEL(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() # * inputs.size(0)\n",
    "\n",
    "                if pd.isna(loss.item()):\n",
    "                    print( \"Val Loss:\" ,loss)\n",
    "                    in_rows = torch.isnan(inputs).any(axis=1)\n",
    "                    out_rows = torch.isnan(outputs).any(axis=-1)\n",
    "                    targets_rows = torch.isnan(targets).any(axis=-1)\n",
    "                    print(inputs[in_rows], outputs[out_rows], targets[targets_rows],sep=\"\\n\")\n",
    "                    return\n",
    "                \n",
    "                # preds = torch.round(torch.sigmoid(outputs))\n",
    "                preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "                val_correct += (preds == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "                f1Score = f1_score(targets, preds)\n",
    "                precision = precision_score(targets,preds,zero_division=.0)\n",
    "                recall = recall_score(targets, preds)\n",
    "                accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "\n",
    "        if val_total == 0:\n",
    "            print(\"Empty validation set!\")\n",
    "            return\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "        val_f1s.append(f1Score)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \"\n",
    "            #   f\"Val Loss: {val_loss:.6f} Acc: {val_acc:.6f}\"\n",
    "              f\"Val Loss: {val_loss:.6f} Acc: {accuracy:.2f}% Precision: {precision:.2f} Recall: {recall:.2f} F1: {f1Score:.2f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_loss_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best loss model saved!\")\n",
    "        if f1Score > best_val_f1:\n",
    "            best_val_f1 = f1Score\n",
    "            best_f1_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best f1 model saved!\")\n",
    "        elif val_loss > best_val_loss and epoch > best_loss_model[\"epoch\"] + patience:\n",
    "            print(f\"Early stopping... Current Val Loss:{val_loss}  vs  Best Val Loss: {best_val_loss}\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='x')\n",
    "    plt.title(\"Training vs. Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.plot(val_accuracies, label='Val Acc', marker='x')\n",
    "    plt.title(\"Training vs. Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_precisions, label='Val Precision', marker='^')\n",
    "    plt.plot(val_recalls, label='Val Recall', marker='x')\n",
    "    plt.plot(val_f1s, label='Val F1', marker='|')\n",
    "    plt.title(\"Precision, Recall, F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    trainingData = {\n",
    "        \"learningRate\": learningRate,\n",
    "        \"epochs\": (epoch, epochs),\n",
    "        \"treshold\": threshold,\n",
    "        \"initial training loss\": train_losses,\n",
    "        \"initial training acc\": train_accuracies,\n",
    "        \"initial validation loss\": val_losses,\n",
    "        \"initial validation acc\": val_accuracies,\n",
    "        \"initial validation precision\": val_precisions,\n",
    "        \"initial validation recall\": val_recalls,\n",
    "        \"initial validation F1\":val_f1s, \n",
    "        \"best initial loss model\": best_loss_model,\n",
    "        \"best initial f1 model\": best_f1_model,\n",
    "    }\n",
    "    \n",
    "    with open(\"loss_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(best_loss_model, f)\n",
    "        print(\"Fiile saved\")\n",
    "        \n",
    "    with open(\"f1_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(best_f1_model, f)\n",
    "        print(\"Fiile saved\")\n",
    "        \n",
    "    return trainingData\n",
    "\n",
    "def train_loss_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, training90_loader, test10_loader, epochs=20, learningRate = 0.00001, threshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading best loss model State from epoch {bestModelData['epoch']}\")\n",
    "    finalModel = model(nFeatures)\n",
    "    finalModel.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    \n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(finalModel.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData[\"epoch\"]):\n",
    "        finalModel.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in training90_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f}\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    finalModel.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test10_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    loss_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": finalModel.state_dict(),\n",
    "    }\n",
    "    \n",
    "    with open(\"Loss_Model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(finalModel.state_dict(), f)\n",
    "        print(\"Loss Model State Dict saved\")\n",
    "\n",
    "    \n",
    "        \n",
    "    return loss_Model\n",
    "\n",
    "def train_f1_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, trainVal_loader, test_loader, learningRate = 0.00001, threshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Loading best f1 model State from epoch {bestModelData['epoch']}\")\n",
    "    f1Model = model(nFeatures)\n",
    "    f1Model.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(f1Model.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData['epoch']):\n",
    "        f1Model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in trainVal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{bestModelData['epoch']}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    f1Model.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "            test_auc = roc_auc_score(targets, preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "                f\"AUC Score: {test_auc:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    f1_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": f1Model.state_dict(),\n",
    "    }\n",
    "    with open(\"f1_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(f1Model.state_dict(), f)\n",
    "        print(\"F1 Model State Dict saved\")\n",
    "    \n",
    "    return f1_Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdaf082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#?  hyperparameter declarations\n",
    "nInputs = len(training80_x.columns)\n",
    "batchSize = 700\n",
    "epoch = 1000\n",
    "learningRate = 0.000001\n",
    "treshold = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivan_NN(\n",
      "  (input_fc): Linear(in_features=28, out_features=2180, bias=True)\n",
      "  (input_bn): BatchNorm1d(2180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      "  (block1): Sequential(\n",
      "    (0): Linear(in_features=2180, out_features=888, bias=True)\n",
      "    (1): BatchNorm1d(888, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Linear(in_features=888, out_features=1122, bias=True)\n",
      "    (1): BatchNorm1d(1122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Linear(in_features=1122, out_features=624, bias=True)\n",
      "    (1): BatchNorm1d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): Linear(in_features=624, out_features=1080, bias=True)\n",
      "    (1): BatchNorm1d(1080, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): Linear(in_features=1080, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=36, bias=True)\n",
      "    (1): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (block8): Sequential(\n",
      "    (0): Linear(in_features=36, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (skip1_proj): Sequential(\n",
      "    (0): Linear(in_features=2180, out_features=1122, bias=True)\n",
      "  )\n",
      "  (skip2_proj): Sequential(\n",
      "    (0): Linear(in_features=1122, out_features=128, bias=True)\n",
      "  )\n",
      "  (skip3_proj): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/1000]   Train Loss: 0.005356 Acc: 0.883553 | Val Loss: 0.002052 Acc: 0.90% Precision: 0.00 Recall: 0.00 F1: 0.00\n",
      "Best loss model saved!\n",
      "Epoch [2/1000]   Train Loss: 0.005069 Acc: 0.883015 | Val Loss: 0.002008 Acc: 0.90% Precision: 0.50 Recall: 0.02 F1: 0.03\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [3/1000]   Train Loss: 0.004795 Acc: 0.882206 | Val Loss: 0.001976 Acc: 0.90% Precision: 0.55 Recall: 0.09 F1: 0.16\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [4/1000]   Train Loss: 0.004599 Acc: 0.880119 | Val Loss: 0.001942 Acc: 0.89% Precision: 0.38 Recall: 0.14 F1: 0.20\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [5/1000]   Train Loss: 0.004435 Acc: 0.876684 | Val Loss: 0.001922 Acc: 0.88% Precision: 0.31 Recall: 0.14 F1: 0.19\n",
      "Best loss model saved!\n",
      "Epoch [6/1000]   Train Loss: 0.004309 Acc: 0.880590 | Val Loss: 0.001902 Acc: 0.88% Precision: 0.34 Recall: 0.19 F1: 0.24\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [7/1000]   Train Loss: 0.004185 Acc: 0.874663 | Val Loss: 0.001879 Acc: 0.87% Precision: 0.31 Recall: 0.22 F1: 0.26\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [8/1000]   Train Loss: 0.004054 Acc: 0.876078 | Val Loss: 0.001872 Acc: 0.88% Precision: 0.33 Recall: 0.22 F1: 0.26\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [9/1000]   Train Loss: 0.003968 Acc: 0.874529 | Val Loss: 0.001860 Acc: 0.87% Precision: 0.32 Recall: 0.23 F1: 0.27\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [10/1000]  Train Loss: 0.003867 Acc: 0.877088 | Val Loss: 0.001862 Acc: 0.88% Precision: 0.33 Recall: 0.23 F1: 0.28\n",
      "Best f1 model saved!\n",
      "Epoch [11/1000]  Train Loss: 0.003806 Acc: 0.875673 | Val Loss: 0.001855 Acc: 0.87% Precision: 0.32 Recall: 0.25 F1: 0.28\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [12/1000]  Train Loss: 0.003745 Acc: 0.871767 | Val Loss: 0.001853 Acc: 0.87% Precision: 0.34 Recall: 0.28 F1: 0.31\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [13/1000]  Train Loss: 0.003692 Acc: 0.872171 | Val Loss: 0.001855 Acc: 0.87% Precision: 0.33 Recall: 0.27 F1: 0.29\n",
      "Epoch [14/1000]  Train Loss: 0.003625 Acc: 0.877020 | Val Loss: 0.001862 Acc: 0.87% Precision: 0.33 Recall: 0.25 F1: 0.28\n",
      "Epoch [15/1000]  Train Loss: 0.003594 Acc: 0.875337 | Val Loss: 0.001860 Acc: 0.87% Precision: 0.31 Recall: 0.28 F1: 0.30\n",
      "Epoch [16/1000]  Train Loss: 0.003533 Acc: 0.872777 | Val Loss: 0.001861 Acc: 0.87% Precision: 0.32 Recall: 0.28 F1: 0.30\n",
      "Epoch [17/1000]  Train Loss: 0.003491 Acc: 0.872037 | Val Loss: 0.001870 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Best f1 model saved!\n",
      "Epoch [18/1000]  Train Loss: 0.003481 Acc: 0.874057 | Val Loss: 0.001885 Acc: 0.87% Precision: 0.33 Recall: 0.28 F1: 0.31\n",
      "Epoch [19/1000]  Train Loss: 0.003446 Acc: 0.873316 | Val Loss: 0.001894 Acc: 0.88% Precision: 0.34 Recall: 0.25 F1: 0.29\n",
      "Epoch [20/1000]  Train Loss: 0.003408 Acc: 0.872980 | Val Loss: 0.001894 Acc: 0.87% Precision: 0.33 Recall: 0.31 F1: 0.32\n",
      "Best f1 model saved!\n",
      "Epoch [21/1000]  Train Loss: 0.003368 Acc: 0.872845 | Val Loss: 0.001905 Acc: 0.88% Precision: 0.35 Recall: 0.27 F1: 0.30\n",
      "Epoch [22/1000]  Train Loss: 0.003355 Acc: 0.873047 | Val Loss: 0.001916 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Epoch [23/1000]  Train Loss: 0.003344 Acc: 0.870353 | Val Loss: 0.001913 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Epoch [24/1000]  Train Loss: 0.003319 Acc: 0.873316 | Val Loss: 0.001925 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Epoch [25/1000]  Train Loss: 0.003316 Acc: 0.873788 | Val Loss: 0.001938 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Epoch [26/1000]  Train Loss: 0.003318 Acc: 0.871767 | Val Loss: 0.001942 Acc: 0.86% Precision: 0.33 Recall: 0.34 F1: 0.34\n",
      "Best f1 model saved!\n",
      "Epoch [27/1000]  Train Loss: 0.003307 Acc: 0.870218 | Val Loss: 0.001956 Acc: 0.87% Precision: 0.33 Recall: 0.34 F1: 0.34\n",
      "Best f1 model saved!\n",
      "Epoch [28/1000]  Train Loss: 0.003255 Acc: 0.871498 | Val Loss: 0.001949 Acc: 0.87% Precision: 0.33 Recall: 0.31 F1: 0.32\n",
      "Epoch [29/1000]  Train Loss: 0.003287 Acc: 0.871430 | Val Loss: 0.001968 Acc: 0.87% Precision: 0.32 Recall: 0.28 F1: 0.30\n",
      "Epoch [30/1000]  Train Loss: 0.003251 Acc: 0.870622 | Val Loss: 0.001976 Acc: 0.87% Precision: 0.34 Recall: 0.34 F1: 0.34\n",
      "Best f1 model saved!\n",
      "Epoch [31/1000]  Train Loss: 0.003276 Acc: 0.871228 | Val Loss: 0.001971 Acc: 0.86% Precision: 0.33 Recall: 0.34 F1: 0.34\n",
      "Epoch [32/1000]  Train Loss: 0.003234 Acc: 0.871363 | Val Loss: 0.001973 Acc: 0.87% Precision: 0.34 Recall: 0.34 F1: 0.34\n",
      "Epoch [33/1000]  Train Loss: 0.003216 Acc: 0.870690 | Val Loss: 0.001978 Acc: 0.87% Precision: 0.34 Recall: 0.34 F1: 0.34\n",
      "Epoch [34/1000]  Train Loss: 0.003224 Acc: 0.873451 | Val Loss: 0.001978 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Epoch [35/1000]  Train Loss: 0.003189 Acc: 0.871565 | Val Loss: 0.001983 Acc: 0.87% Precision: 0.32 Recall: 0.31 F1: 0.32\n",
      "Epoch [36/1000]  Train Loss: 0.003208 Acc: 0.869006 | Val Loss: 0.001990 Acc: 0.86% Precision: 0.33 Recall: 0.34 F1: 0.34\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#? main model training and validation loop\n",
    "trainingData = train_model(Ivan_NN, nInputs, train80_loader, val10_loader, epochs=epoch, learningRate=learningRate, threshold=treshold, patience=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#? model training and test loop\n",
    "best_loss_model = pickle.load(open(\"f1_model.pkl\", \"rb\"))\n",
    "testingData = train_f1_model(Ivan_NN, nInputs, best_loss_model, training90_loader, test10_loader,learningRate=learningRate, threshold=treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4243b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
