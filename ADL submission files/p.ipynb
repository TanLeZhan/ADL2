{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fbeb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "from dataset.data_preprocessing import *\n",
    "from Preprocessing_Functions2 import *\n",
    "from Training_Helper_Functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4b0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed (one-hot) data saved to processed_data_OHE.csv\n",
      "Processed (label-encoded) data saved to processed_data_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "load_and_preprocess_data(\"dataset/raw_data.csv\") #? convert raw data to OHE and encoded version\n",
    "data = pd.read_csv(\"dataset/processed_data_encoded.csv\")\n",
    "\n",
    "#? Split training and holdout/test set\n",
    "trainVal, testing = train_test_split(data, test_size=0.1, random_state=42, stratify=data[\"DR\"])\n",
    "\n",
    "#? Split training and validation set\n",
    "training, validation = train_test_split(trainVal, test_size=1/9, random_state=42, stratify=trainVal[\"DR\"])\n",
    "\n",
    "def CV_generator(training:pd.DataFrame, validation:pd.DataFrame, OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler()):\n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', \n",
    "             'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    X = training.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "    validation = validation.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "\n",
    "    #* OUTLIER DETECTION\n",
    "    X_train_processed = Outlier_Removal(X, \n",
    "                                    OD_majority=OD_majority,\n",
    "                                    OD_minority=OD_minority,\n",
    "    )\n",
    "    # #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "    print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "    #* Smote but we don't use it for training\n",
    "    # _train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "    X_train_processed = Synthetic_Data_Generator2(X_train_processed, \"\", synthesizer=synthesizer, epochs=epochs, batch_size=256, n_synthetic_data=n_synthetic_data)\n",
    "    print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "    \n",
    "    #* Calculate BMI, TCTG & ENCODING\n",
    "    X_train_processed, validation = get_bmi(X_train_processed, validation)\n",
    "    X_train_processed, validation = get_TCTG(X_train_processed, validation)\n",
    "    X_train_processed, validation = apply_one_hot_encoding(X_train_processed, validation)\n",
    "    #* Scaler\n",
    "    X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "    validation[cont_cols] = scaler.transform(validation[cont_cols])\n",
    "    # Append processed data (excluding the target column 'DR')\n",
    "\n",
    "    # Save to CSV with fold number\n",
    "    X_train_processed.to_csv(f\"./DATA/training80.csv\", index=False)\n",
    "    validation.to_csv(f\"./DATA/val10.csv\", index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "def final_generator(training:pd.DataFrame, test:pd.DataFrame,\n",
    "OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler()):\n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', \n",
    "             'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    X = training.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "    test = test.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "\n",
    "    #* OUTLIER DETECTION\n",
    "    X_train_processed = Outlier_Removal(X, \n",
    "                                    OD_majority=OD_majority,\n",
    "                                    OD_minority=OD_minority,\n",
    "    )\n",
    "    # #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "    print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "    #* Smote but we don't use it for training\n",
    "    # _train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "    X_train_processed = Synthetic_Data_Generator2(X_train_processed, \"\", synthesizer=synthesizer, epochs=epochs, batch_size=256, n_synthetic_data=n_synthetic_data)\n",
    "    print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "    \n",
    "    #* Calculate BMI, TCTG & ENCODING\n",
    "    X_train_processed, test = get_bmi(X_train_processed, test)\n",
    "    X_train_processed, test = get_TCTG(X_train_processed, test)\n",
    "    X_train_processed, test = apply_one_hot_encoding(X_train_processed, test)\n",
    "    #* Scaler\n",
    "    X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "    test[cont_cols] = scaler.transform(test[cont_cols])\n",
    "    # Append processed data (excluding the target column 'DR')\n",
    "\n",
    "    # Save to CSV with fold number\n",
    "    X_train_processed.to_csv(f\"./DATA/training90.csv\", index=False)\n",
    "    test.to_csv(f\"./DATA/test10.csv\", index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3537158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: DR\n",
      "0.0    4588\n",
      "1.0     516\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    4588\n",
      "1.0     516\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "  File \"d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Loss: -10.902: 100%|██████████| 1000/1000 [28:41<00:00,  1.72s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 8989/8989 [00:00<00:00, 10919.46it/s]\n",
      "Sampling conditions: 100%|██████████| 1010/1010 [00:03<00:00, 255.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    8989\n",
      "1.0    1010\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 18/18 [00:03<00:00,  4.75it/s]|\n",
      "Column Shapes Score: 90.59%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 153/153 [00:01<00:00, 100.35it/s]|\n",
      "Column Pair Trends Score: 86.74%\n",
      "\n",
      "Overall Score (Average): 88.67%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    13577\n",
      "1.0     1526\n",
      "Name: count, dtype: int64\n",
      "Original class distribution: DR\n",
      "0.0    4588\n",
      "1.0     516\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    4588\n",
      "1.0     516\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -11.023: 100%|██████████| 1000/1000 [14:28<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 8989/8989 [00:01<00:00, 7614.20it/s] \n",
      "Sampling conditions: 100%|██████████| 1010/1010 [00:03<00:00, 268.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    8989\n",
      "1.0    1010\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 18/18 [00:02<00:00,  6.56it/s]|\n",
      "Column Shapes Score: 91.3%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 153/153 [00:01<00:00, 95.21it/s]| \n",
      "Column Pair Trends Score: 86.96%\n",
      "\n",
      "Overall Score (Average): 89.13%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    13577\n",
      "1.0     1526\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m test10_x = test10.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mDR\u001b[39m\u001b[33m\"\u001b[39m]) \n\u001b[32m     18\u001b[39m test10_y = test10[[\u001b[33m\"\u001b[39m\u001b[33mDR\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m train80_loader, val10_loader = \u001b[43mfold_to_dataloader_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining80_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining80_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval10_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval10_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m training90_loader, test10_loader = fold_to_dataloader_tensor(training90_x, training90_y, test10_x, test10_y, batch_size=\u001b[32m256\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\ADL submission files\\Training_Helper_Functions.py:24\u001b[39m, in \u001b[36mfold_to_dataloader_tensor\u001b[39m\u001b[34m(train_x, test_x, train_y, test_y, batch_size, device)\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❌ NaNs detected in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after conversion in columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(bad_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create PyTorch datasets and loaders\u001b[39;00m\n\u001b[32m     22\u001b[39m train_dataset = TensorDataset(\n\u001b[32m     23\u001b[39m                             torch.tensor(train_x.values, dtype=torch.float32).to(device),\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m                             \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     25\u001b[39m                         )\n\u001b[32m     26\u001b[39m val_dataset = TensorDataset(\n\u001b[32m     27\u001b[39m                         torch.tensor(test_x.values, dtype=torch.float32).to(device),\n\u001b[32m     28\u001b[39m                         torch.tensor(test_y.values, dtype=torch.float32).view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m).to(device)\n\u001b[32m     29\u001b[39m                     )\n\u001b[32m     30\u001b[39m train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# CV_generator(training, validation, OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler())\n",
    "\n",
    "CV_generator(training, validation,\n",
    "            OD_majority = IQRDetector(factor=1.2),\n",
    "            OD_minority = IQRDetector(factor=1.8),\n",
    "            synthesizer = \"TVAE\",\n",
    "            epochs = 1000,\n",
    "            n_synthetic_data = 10000,\n",
    "            scaler=StandardScaler())\n",
    "\n",
    "# final_generator(training, testing, OD_majority=None, OD_minority=None,synthesizer = \"TVAE\", epochs = 1000, n_synthetic_data=10000, scaler=StandardScaler())\n",
    "\n",
    "final_generator(training, testing,\n",
    "                OD_majority = IQRDetector(factor=1.2),\n",
    "                OD_minority = IQRDetector(factor=1.8),\n",
    "                synthesizer = \"TVAE\",\n",
    "                epochs = 1000,\n",
    "                n_synthetic_data = 10000,\n",
    "                scaler=StandardScaler())\n",
    "\n",
    ")\n",
    "training80 = pd.read_csv(\"./DATA/training80.csv\")\n",
    "val10 = pd.read_csv(\"./DATA/val10.csv\")\n",
    "training90 = pd.read_csv(\"./DATA/training90.csv\")\n",
    "test10 = pd.read_csv(\"./DATA/test10.csv\")\n",
    "\n",
    "training80_x = training80.drop(columns=[\"DR\"])\n",
    "training80_y = training80[[\"DR\"]]\n",
    "val10_x = val10.drop(columns=[\"DR\"]) \n",
    "val10_y = val10[[\"DR\"]]\n",
    "\n",
    "training90_x = training90.drop(columns=[\"DR\"])\n",
    "training90_y = training90[[\"DR\"]]\n",
    "test10_x = test10.drop(columns=[\"DR\"]) \n",
    "test10_y = test10[[\"DR\"]]\n",
    "\n",
    "train80_loader, val10_loader = fold_to_dataloader_tensor(training80_x, training80_y, val10_x, val10_y, batch_size=256)\n",
    "\n",
    "training90_loader, test10_loader = fold_to_dataloader_tensor(training90_x, training90_y, test10_x, test10_y, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Ivan_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, 2180)\n",
    "        self.input_bn = nn.BatchNorm1d(2180)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(2180, 888),\n",
    "            nn.BatchNorm1d(888),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(888, 1122),\n",
    "            nn.BatchNorm1d(1122),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(1122, 624),\n",
    "            nn.BatchNorm1d(624),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(624, 1080),\n",
    "            nn.BatchNorm1d(1080),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(1080, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),  # ← Fixed this\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Linear(128, 36),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Linear(36, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),  # or LeakyReLU, up to you\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Skip connection projectors\n",
    "        self.skip1_proj = nn.Sequential(nn.Linear(2180, 1122))\n",
    "        self.skip2_proj = nn.Sequential(nn.Linear(1122, 128))\n",
    "        self.skip3_proj = nn.Sequential(nn.Linear(128, 64))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.input_bn(x)\n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "\n",
    "        skip1 = self.skip1_proj(x)\n",
    "        x2 = x2 + skip1\n",
    "\n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        x5 = self.block5(x4)\n",
    "\n",
    "        x6 = self.block6(x5)\n",
    "\n",
    "        skip2 = self.skip2_proj(x2)\n",
    "        x6 = x6 + skip2\n",
    "\n",
    "        x7 = self.block7(x6)\n",
    "        x8 = self.block8(x7)\n",
    "\n",
    "        skip3 = self.skip3_proj(x6)\n",
    "        x8 = x8 + skip3\n",
    "\n",
    "        out = self.output(x8)\n",
    "        return out\n",
    "\n",
    "def train_model(model:Ivan_NN, nFeatures:int, train80_loader, val10_loader, epochs=10, learningRate = 0.0001, threshold:float = 0.5, patience = 30, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    theMODEL = model(nFeatures)\n",
    "    theMODEL.to(device)\n",
    "    print(theMODEL)\n",
    "\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(theMODEL.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies= []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_f1s = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_f1 = .0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        theMODEL.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in train80_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = theMODEL(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        theMODEL.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val10_loader:\n",
    "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "                outputs = theMODEL(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() # * inputs.size(0)\n",
    "\n",
    "                if pd.isna(loss.item()):\n",
    "                    print( \"Val Loss:\" ,loss)\n",
    "                    in_rows = torch.isnan(inputs).any(axis=1)\n",
    "                    out_rows = torch.isnan(outputs).any(axis=-1)\n",
    "                    targets_rows = torch.isnan(targets).any(axis=-1)\n",
    "                    print(inputs[in_rows], outputs[out_rows], targets[targets_rows],sep=\"\\n\")\n",
    "                    return\n",
    "                \n",
    "                # preds = torch.round(torch.sigmoid(outputs))\n",
    "                preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "                val_correct += (preds == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "                f1Score = f1_score(targets, preds)\n",
    "                precision = precision_score(targets,preds,zero_division=.0)\n",
    "                recall = recall_score(targets, preds)\n",
    "                accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "\n",
    "        if val_total == 0:\n",
    "            print(\"Empty validation set!\")\n",
    "            return\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "        val_f1s.append(f1Score)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \"\n",
    "            #   f\"Val Loss: {val_loss:.6f} Acc: {val_acc:.6f}\"\n",
    "              f\"Val Loss: {val_loss:.6f} Acc: {accuracy:.2f}% Precision: {precision:.2f} Recall: {recall:.2f} F1: {f1Score:.2f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_loss_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best loss model saved!\")\n",
    "        if f1Score > best_val_f1:\n",
    "            best_val_f1 = f1Score\n",
    "            best_f1_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best f1 model saved!\")\n",
    "        elif val_loss > best_val_loss and epoch > best_loss_model[\"epoch\"] + patience:\n",
    "            print(f\"Early stopping... Current Val Loss:{val_loss}  vs  Best Val Loss: {best_val_loss}\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='x')\n",
    "    plt.title(\"Training vs. Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.plot(val_accuracies, label='Val Acc', marker='x')\n",
    "    plt.title(\"Training vs. Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_precisions, label='Val Precision', marker='^')\n",
    "    plt.plot(val_recalls, label='Val Recall', marker='x')\n",
    "    plt.plot(val_f1s, label='Val F1', marker='|')\n",
    "    plt.title(\"Precision, Recall, F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    trainingData = {\n",
    "        \"learningRate\": learningRate,\n",
    "        \"epochs\": (epoch, epochs),\n",
    "        \"treshold\": threshold,\n",
    "        \"initial training loss\": train_losses,\n",
    "        \"initial training acc\": train_accuracies,\n",
    "        \"initial validation loss\": val_losses,\n",
    "        \"initial validation acc\": val_accuracies,\n",
    "        \"initial validation precision\": val_precisions,\n",
    "        \"initial validation recall\": val_recalls,\n",
    "        \"initial validation F1\":val_f1s, \n",
    "        \"best initial loss model\": best_loss_model,\n",
    "        \"best initial f1 model\": best_f1_model,\n",
    "    }\n",
    "    \n",
    "    with open(\"loss_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(best_loss_model, f)\n",
    "        print(\"Fiile saved\")\n",
    "        \n",
    "    with open(\"f1_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(best_f1_model, f)\n",
    "        print(\"Fiile saved\")\n",
    "        \n",
    "    return trainingData\n",
    "\n",
    "def train_loss_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, training90_loader, test10_loader, epochs=20, learningRate = 0.00001, threshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading best loss model State from epoch {bestModelData['epoch']}\")\n",
    "    finalModel = model(nFeatures)\n",
    "    finalModel.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    \n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(finalModel.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData[\"epoch\"]):\n",
    "        finalModel.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in training90_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f}\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    finalModel.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test10_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    loss_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": finalModel.state_dict(),\n",
    "    }\n",
    "    \n",
    "    with open(\"Loss_Model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(finalModel.state_dict(), f)\n",
    "        print(\"Loss Model State Dict saved\")\n",
    "\n",
    "    \n",
    "        \n",
    "    return loss_Model\n",
    "\n",
    "def train_f1_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, trainVal_loader, test_loader, learningRate = 0.00001, threshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Loading best f1 model State from epoch {bestModelData['epoch']}\")\n",
    "    f1Model = model(nFeatures)\n",
    "    f1Model.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(f1Model.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData['epoch']):\n",
    "        f1Model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in trainVal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{bestModelData['epoch']}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    f1Model.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= threshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "            test_auc = roc_auc_score(targets, preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "                f\"AUC Score: {test_auc:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    f1_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": f1Model.state_dict(),\n",
    "    }\n",
    "    with open(\"f1_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(f1Model.state_dict(), f)\n",
    "        print(\"F1 Model State Dict saved\")\n",
    "    \n",
    "    return f1_Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#?  hyperparameter declarations\n",
    "nInputs = len(training80_x.columns)\n",
    "batchSize = 700\n",
    "epoch = 1000\n",
    "learningRate = 0.000001\n",
    "treshold = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e3200",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#? main model training and validation loop\n",
    "trainingData = train_model(Ivan_NN, nInputs, train80_loader, val10_loader, epochs=epoch, learningRate=learningRate, threshold=treshold, patience=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#? model training and test loop\n",
    "best_loss_model = pickle.load(open(\"f1_model.pkl\", \"rb\"))\n",
    "testingData = train_f1_model(Ivan_NN, nInputs, best_loss_model, training90_loader, test10_loader,learningRate=learningRate, threshold=treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4243b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
