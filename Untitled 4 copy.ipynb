{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Ivan_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, 2180)\n",
    "        self.input_bn = nn.BatchNorm1d(2180)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(2180, 888),\n",
    "            nn.BatchNorm1d(888),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(888, 1122),\n",
    "            nn.BatchNorm1d(1122),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(1122, 624),\n",
    "            nn.BatchNorm1d(624),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(624, 1080),\n",
    "            nn.BatchNorm1d(1080),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(1080, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),  # ← Fixed this\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Linear(128, 36),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Linear(36, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),  # or LeakyReLU, up to you\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Skip connection projectors\n",
    "        self.skip1_proj = nn.Sequential(nn.Linear(2180, 1122))\n",
    "        self.skip2_proj = nn.Sequential(nn.Linear(1122, 128))\n",
    "        self.skip3_proj = nn.Sequential(nn.Linear(128, 64))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.input_bn(x)\n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "\n",
    "        skip1 = self.skip1_proj(x)\n",
    "        x2 = x2 + skip1\n",
    "\n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        x5 = self.block5(x4)\n",
    "\n",
    "        x6 = self.block6(x5)\n",
    "\n",
    "        skip2 = self.skip2_proj(x2)\n",
    "        x6 = x6 + skip2\n",
    "\n",
    "        x7 = self.block7(x6)\n",
    "        x8 = self.block8(x7)\n",
    "\n",
    "        skip3 = self.skip3_proj(x6)\n",
    "        x8 = x8 + skip3\n",
    "\n",
    "        out = self.output(x8)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.DataFrame):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model:Ivan_NN, nFeatures:int, train_loader, val_loader, epochs=10, learningRate = 0.0001, treshold:float = 0.5, patience = 30, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    theMODEL = model(nFeatures)\n",
    "    theMODEL.to(device)\n",
    "    print(theMODEL)\n",
    "\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(theMODEL.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies= []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_f1s = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_f1 = .0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        theMODEL.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = theMODEL(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() # * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        theMODEL.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "                outputs = theMODEL(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() # * inputs.size(0)\n",
    "\n",
    "                if pd.isna(loss.item()):\n",
    "                    print( \"Val Loss:\" ,loss)\n",
    "                    in_rows = torch.isnan(inputs).any(axis=1)\n",
    "                    out_rows = torch.isnan(outputs).any(axis=-1)\n",
    "                    targets_rows = torch.isnan(targets).any(axis=-1)\n",
    "                    print(inputs[in_rows], outputs[out_rows], targets[targets_rows],sep=\"\\n\")\n",
    "                    return\n",
    "                \n",
    "                # preds = torch.round(torch.sigmoid(outputs))\n",
    "                preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "                val_correct += (preds == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "                f1Score = f1_score(targets, preds)\n",
    "                precision = precision_score(targets,preds,zero_division=.0)\n",
    "                recall = recall_score(targets, preds)\n",
    "                accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "\n",
    "        if val_total == 0:\n",
    "            print(\"Empty validation set!\")\n",
    "            return\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "        val_f1s.append(f1Score)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \"\n",
    "            #   f\"Val Loss: {val_loss:.6f} Acc: {val_acc:.6f}\"\n",
    "              f\"Val Loss: {val_loss:.6f} Acc: {accuracy:.2f}% Precision: {precision:.2f} Recall: {recall:.2f} F1: {f1Score:.2f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_loss_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best loss model saved!\")\n",
    "        if f1Score > best_val_f1:\n",
    "            best_val_f1 = f1Score\n",
    "            best_f1_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best f1 model saved!\")\n",
    "        elif val_loss > best_val_loss and epoch > best_loss_model[\"epoch\"] + patience:\n",
    "            print(f\"Early stopping... Current Val Loss:{val_loss}  vs  Best Val Loss: {best_val_loss}\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='x')\n",
    "    plt.title(\"Training vs. Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.plot(val_accuracies, label='Val Acc', marker='x')\n",
    "    plt.title(\"Training vs. Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_precisions, label='Val Precision', marker='^')\n",
    "    plt.plot(val_recalls, label='Val Recall', marker='x')\n",
    "    plt.plot(val_f1s, label='Val F1', marker='|')\n",
    "    plt.title(\"Precision, Recall, F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    trainingData = {\n",
    "        \"learningRate\": learningRate,\n",
    "        \"epochs\": (epoch, epochs),\n",
    "        \"treshold\": treshold,\n",
    "        \"initial training loss\": train_losses,\n",
    "        \"initial training acc\": train_accuracies,\n",
    "        \"initial validation loss\": val_losses,\n",
    "        \"initial validation acc\": val_accuracies,\n",
    "        \"initial validation precision\": val_precisions,\n",
    "        \"initial validation recall\": val_recalls,\n",
    "        \"initial validation F1\":val_f1s, \n",
    "        \"best initial loss model\": best_loss_model,\n",
    "        \"best initial f1 model\": best_f1_model,\n",
    "    }\n",
    "    \n",
    "    with open(\"loss_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(best_loss_model, f)\n",
    "        print(\"Fiile saved\")\n",
    "        \n",
    "    with open(\"f1_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(best_f1_model, f)\n",
    "        print(\"Fiile saved\")\n",
    "        \n",
    "    return trainingData\n",
    "\n",
    "    \n",
    "def train_loss_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, trainVal_loader, test_loader, epochs=20, learningRate = 0.00001, treshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading best loss model State from epoch {bestModelData['epoch']}\")\n",
    "    finalModel = model(nFeatures)\n",
    "    finalModel.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    \n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(finalModel.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData[\"epoch\"]):\n",
    "        finalModel.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in trainVal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f}\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    finalModel.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    loss_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": finalModel.state_dict(),\n",
    "    }\n",
    "    \n",
    "    with open(\"FinalModelStateDict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(finalModel.state_dict(), f)\n",
    "        print(\"Model State Dict saved\")\n",
    "\n",
    "    \n",
    "        \n",
    "    return loss_Model\n",
    "\n",
    "def train_f1_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, trainVal_loader, test_loader, epochs=20, learningRate = 0.00001, treshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Loading best f1 model State from epoch {bestModelData['epoch']}\")\n",
    "    f1Model = model(nFeatures)\n",
    "    f1Model.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(f1Model.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData['epoch']):\n",
    "        f1Model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in trainVal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    f1Model.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    f1_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": f1Model.state_dict(),\n",
    "    }\n",
    "\n",
    "    \n",
    "    return f1_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2560759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory vanData created\n",
      "Original class distribution: DR\n",
      "0.0    4128\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    4128\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\SUTD\\Term 6\\Applied Deep Learning\\Project\\ADL2\\venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -21.132: 100%|██████████| 10000/10000 [1:51:57<00:00,  1.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 8989/8989 [00:01<00:00, 6854.42it/s]\n",
      "Sampling conditions: 100%|██████████| 1010/1010 [00:04<00:00, 248.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    8989\n",
      "1.0    1010\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 20/20 [00:06<00:00,  3.25it/s]|\n",
      "Column Shapes Score: 90.2%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 190/190 [00:02<00:00, 84.53it/s]|\n",
      "Column Pair Trends Score: 88.31%\n",
      "\n",
      "Overall Score (Average): 89.26%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    13117\n",
      "1.0     1474\n",
      "Name: count, dtype: int64\n",
      "Saving generated training and validation datasets\n",
      "Original class distribution: DR\n",
      "0.0    4645\n",
      "1.0     522\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    4645\n",
      "1.0     522\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\SUTD\\Term 6\\Applied Deep Learning\\Project\\ADL2\\venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -21.023: 100%|██████████| 10000/10000 [2:07:39<00:00,  1.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 8989/8989 [00:00<00:00, 27026.80it/s]\n",
      "Sampling conditions: 100%|██████████| 1010/1010 [00:01<00:00, 644.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    8989\n",
      "1.0    1010\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 20/20 [00:01<00:00, 17.51it/s]|\n",
      "Column Shapes Score: 90.96%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 190/190 [00:00<00:00, 321.34it/s]|\n",
      "Column Pair Trends Score: 88.62%\n",
      "\n",
      "Overall Score (Average): 89.79%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    13634\n",
      "1.0     1532\n",
      "Name: count, dtype: int64\n",
      "Saving generated combinedTrain and test datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Helper_Functions import *\n",
    "from Preprocessing_Functions2 import * \n",
    "path = \"../ADL2/DATA_21/training_set/training_data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# data = cleanRawData()\n",
    "X , Y = data.drop(columns=[\"DR\"]).astype(float), data[[\"DR\"]].astype(float)\n",
    "\n",
    "trainVal, testing = train_test_split(data, test_size=0.1, random_state=42, stratify=data[\"DR\"])\n",
    "training, validation = train_test_split(trainVal, test_size=1/9, random_state=42, stratify=trainVal[\"DR\"])\n",
    "\n",
    "# trainVal_X, test_X, trainVal_Y, test_Y = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "# train_X, val_X, train_Y, val_Y = train_test_split(trainVal_X, trainVal_Y, test_size=1/9, random_state=42, stratify=trainVal_Y)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "scaler = StandardScaler()\n",
    "cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', \n",
    "             'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "cat_cols = ['Gender', 'Community'] \n",
    "y_col = 'DR'\n",
    "\n",
    "# Create the directory\n",
    "directory = \"vanData\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "print(f\"directory {directory} created\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#* OUTLIER DETECTION\n",
    "X_train_processed = Outlier_Removal(training, \n",
    "                                    OD_majority=None,\n",
    "                                    OD_minority=None,\n",
    "                                    )\n",
    "# X_train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "# #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "X_train_processed = Synthetic_Data_Generator2(X_train_processed, \"\", synthesizer=\"TVAE\", epochs=10000, batch_size=700, n_synthetic_data=10000)\n",
    "\n",
    "print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "#* Calculate BMI, TCTG & ENCODING\n",
    "X_train_processed, validation = get_bmi(X_train_processed, validation)\n",
    "X_train_processed, validation = get_TCTG(X_train_processed, validation)\n",
    "X_train_processed, validation = apply_one_hot_encoding(X_train_processed, validation)\n",
    "#* Scaler\n",
    "X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "validation[cont_cols] = scaler.transform(validation[cont_cols])\n",
    "\n",
    "print(\"Saving generated training and validation datasets\")\n",
    "X_train_processed.to_csv(f\"{directory}/training.csv\")\n",
    "validation.to_csv(f\"{directory}/validation.csv\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# #* OUTLIER DETECTION\n",
    "fullTrain_processed = Outlier_Removal(trainVal, \n",
    "                                    OD_majority=None,\n",
    "                                    OD_minority=None,\n",
    "                                    )\n",
    "# X_train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "# #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "print(\"Before oversampling & synthetic data:\", fullTrain_processed[[\"DR\"]].value_counts())\n",
    "fullTrain_processed = Synthetic_Data_Generator2(fullTrain_processed, \"\", synthesizer=\"TVAE\", epochs=10000, batch_size=700, n_synthetic_data=10000)\n",
    "\n",
    "print(\"After oversampling & synthetic data:\", fullTrain_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "#* Calculate BMI, TCTG & ENCODING\n",
    "fullTrain_processed, testing = get_bmi(fullTrain_processed, testing)\n",
    "fullTrain_processed, testing = get_TCTG(fullTrain_processed, testing)\n",
    "fullTrain_processed, testing = apply_one_hot_encoding(fullTrain_processed, testing)\n",
    "#* Scaler\n",
    "fullTrain_processed[cont_cols] = scaler.fit_transform(fullTrain_processed[cont_cols])\n",
    "testing[cont_cols] = scaler.transform(testing[cont_cols])\n",
    "\n",
    "print(\"Saving generated combinedTrain and test datasets\")\n",
    "fullTrain_processed.to_csv(f\"{directory}/fullTrain.csv\")\n",
    "testing.to_csv(f\"{directory}/testing.csv\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f7bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#? For Training and Cross Validation Loop\n",
    "train_X_n, train_Y = fullTrain_processed.drop(columns=['DR']), fullTrain_processed[['DR']]\n",
    "# train_X_n = (train_X - train_X.min()) / (train_X.max() - train_X.min())\n",
    "train_dataset = TabularDataset(train_X_n, train_Y)\n",
    "\n",
    "val_X_n , val_Y = validation.drop(columns=['DR']), validation[['DR']]\n",
    "# val_X_n = (val_X - val_X.min()) / (val_X.max() - val_X.min())\n",
    "val_dataset = TabularDataset(val_X_n, val_Y)\n",
    "\n",
    "#? For Final Training and Testing Loop\n",
    "trainVal_X_n , trainVal_Y = fullTrain_processed.drop(columns=['DR']), fullTrain_processed[['DR']]\n",
    "# trainVal_X_n = (trainVal_X - trainVal_X.min()) / (trainVal_X.max() - trainVal_X.min())\n",
    "trainVal_dataset = TabularDataset(trainVal_X_n, trainVal_Y)\n",
    "\n",
    "test_X_n , test_Y = testing.drop(columns=['DR']), testing[['DR']]\n",
    "# test_X_n = (test_X - test_X.min()) / (test_X.max() - test_X.min())\n",
    "test_dataset = TabularDataset(test_X_n, test_Y)\n",
    "\n",
    "nInputs = len(train_X_n.columns)\n",
    "batchSize = 700\n",
    "epoch = 1000\n",
    "learningRate = 0.000001\n",
    "treshold = 0.65\n",
    "# model = Ivan_NN2(nInputs)\n",
    "\n",
    "#? Attemps to \"Balance\" the class imbalance\n",
    "# posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "# optimiser = optim.Adam(model.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, drop_last=True)\n",
    "trainVal_loader = DataLoader(trainVal_dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivan_NN(\n",
      "  (input_fc): Linear(in_features=28, out_features=2180, bias=True)\n",
      "  (input_bn): BatchNorm1d(2180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      "  (block1): Sequential(\n",
      "    (0): Linear(in_features=2180, out_features=888, bias=True)\n",
      "    (1): BatchNorm1d(888, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Linear(in_features=888, out_features=1122, bias=True)\n",
      "    (1): BatchNorm1d(1122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Linear(in_features=1122, out_features=624, bias=True)\n",
      "    (1): BatchNorm1d(624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): Linear(in_features=624, out_features=1080, bias=True)\n",
      "    (1): BatchNorm1d(1080, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): Linear(in_features=1080, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block6): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block7): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=36, bias=True)\n",
      "    (1): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (block8): Sequential(\n",
      "    (0): Linear(in_features=36, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (skip1_proj): Sequential(\n",
      "    (0): Linear(in_features=2180, out_features=1122, bias=True)\n",
      "  )\n",
      "  (skip2_proj): Sequential(\n",
      "    (0): Linear(in_features=1122, out_features=128, bias=True)\n",
      "  )\n",
      "  (skip3_proj): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/1000]   Train Loss: 0.002010 Acc: 0.897483 | Val Loss: 0.002385 Acc: 0.90% Precision: 0.00 Recall: 0.00 F1: 0.00\n",
      "Best loss model saved!\n",
      "Epoch [2/1000]   Train Loss: 0.001945 Acc: 0.897075 | Val Loss: 0.002320 Acc: 0.90% Precision: 0.00 Recall: 0.00 F1: 0.00\n",
      "Best loss model saved!\n",
      "Epoch [3/1000]   Train Loss: 0.001897 Acc: 0.895442 | Val Loss: 0.002277 Acc: 0.89% Precision: 0.17 Recall: 0.02 F1: 0.03\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [4/1000]   Train Loss: 0.001841 Acc: 0.895102 | Val Loss: 0.002257 Acc: 0.89% Precision: 0.20 Recall: 0.03 F1: 0.06\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [5/1000]   Train Loss: 0.001808 Acc: 0.894082 | Val Loss: 0.002227 Acc: 0.89% Precision: 0.21 Recall: 0.05 F1: 0.08\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [6/1000]   Train Loss: 0.001777 Acc: 0.890068 | Val Loss: 0.002207 Acc: 0.89% Precision: 0.27 Recall: 0.07 F1: 0.11\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [7/1000]   Train Loss: 0.001744 Acc: 0.889048 | Val Loss: 0.002189 Acc: 0.88% Precision: 0.25 Recall: 0.09 F1: 0.13\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [8/1000]   Train Loss: 0.001722 Acc: 0.884286 | Val Loss: 0.002172 Acc: 0.88% Precision: 0.27 Recall: 0.12 F1: 0.17\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [9/1000]   Train Loss: 0.001706 Acc: 0.884286 | Val Loss: 0.002165 Acc: 0.88% Precision: 0.28 Recall: 0.12 F1: 0.17\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [10/1000]  Train Loss: 0.001679 Acc: 0.882109 | Val Loss: 0.002147 Acc: 0.87% Precision: 0.29 Recall: 0.17 F1: 0.22\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [11/1000]  Train Loss: 0.001649 Acc: 0.883537 | Val Loss: 0.002137 Acc: 0.87% Precision: 0.27 Recall: 0.17 F1: 0.21\n",
      "Best loss model saved!\n",
      "Epoch [12/1000]  Train Loss: 0.001646 Acc: 0.874762 | Val Loss: 0.002125 Acc: 0.86% Precision: 0.24 Recall: 0.17 F1: 0.20\n",
      "Best loss model saved!\n",
      "Epoch [13/1000]  Train Loss: 0.001614 Acc: 0.880748 | Val Loss: 0.002114 Acc: 0.86% Precision: 0.26 Recall: 0.21 F1: 0.23\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [14/1000]  Train Loss: 0.001611 Acc: 0.872585 | Val Loss: 0.002106 Acc: 0.86% Precision: 0.29 Recall: 0.26 F1: 0.28\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [15/1000]  Train Loss: 0.001590 Acc: 0.872993 | Val Loss: 0.002098 Acc: 0.85% Precision: 0.26 Recall: 0.26 F1: 0.26\n",
      "Best loss model saved!\n",
      "Epoch [16/1000]  Train Loss: 0.001583 Acc: 0.873061 | Val Loss: 0.002094 Acc: 0.86% Precision: 0.28 Recall: 0.26 F1: 0.27\n",
      "Best loss model saved!\n",
      "Epoch [17/1000]  Train Loss: 0.001582 Acc: 0.869864 | Val Loss: 0.002083 Acc: 0.85% Precision: 0.28 Recall: 0.29 F1: 0.29\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [18/1000]  Train Loss: 0.001557 Acc: 0.869524 | Val Loss: 0.002079 Acc: 0.85% Precision: 0.28 Recall: 0.29 F1: 0.29\n",
      "Best loss model saved!\n",
      "Epoch [19/1000]  Train Loss: 0.001548 Acc: 0.866395 | Val Loss: 0.002070 Acc: 0.85% Precision: 0.29 Recall: 0.31 F1: 0.30\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [20/1000]  Train Loss: 0.001543 Acc: 0.866190 | Val Loss: 0.002068 Acc: 0.85% Precision: 0.29 Recall: 0.31 F1: 0.30\n",
      "Best loss model saved!\n",
      "Epoch [21/1000]  Train Loss: 0.001522 Acc: 0.869048 | Val Loss: 0.002062 Acc: 0.85% Precision: 0.28 Recall: 0.31 F1: 0.30\n",
      "Best loss model saved!\n",
      "Epoch [22/1000]  Train Loss: 0.001514 Acc: 0.866531 | Val Loss: 0.002054 Acc: 0.84% Precision: 0.26 Recall: 0.33 F1: 0.29\n",
      "Best loss model saved!\n",
      "Epoch [23/1000]  Train Loss: 0.001504 Acc: 0.865170 | Val Loss: 0.002051 Acc: 0.84% Precision: 0.26 Recall: 0.31 F1: 0.28\n",
      "Best loss model saved!\n",
      "Epoch [24/1000]  Train Loss: 0.001499 Acc: 0.863469 | Val Loss: 0.002045 Acc: 0.84% Precision: 0.27 Recall: 0.33 F1: 0.29\n",
      "Best loss model saved!\n",
      "Epoch [25/1000]  Train Loss: 0.001488 Acc: 0.863129 | Val Loss: 0.002043 Acc: 0.84% Precision: 0.26 Recall: 0.33 F1: 0.29\n",
      "Best loss model saved!\n",
      "Epoch [26/1000]  Train Loss: 0.001490 Acc: 0.861088 | Val Loss: 0.002041 Acc: 0.84% Precision: 0.26 Recall: 0.33 F1: 0.29\n",
      "Best loss model saved!\n",
      "Epoch [27/1000]  Train Loss: 0.001476 Acc: 0.862177 | Val Loss: 0.002037 Acc: 0.84% Precision: 0.27 Recall: 0.34 F1: 0.30\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [28/1000]  Train Loss: 0.001471 Acc: 0.863741 | Val Loss: 0.002030 Acc: 0.83% Precision: 0.27 Recall: 0.36 F1: 0.31\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [29/1000]  Train Loss: 0.001468 Acc: 0.859932 | Val Loss: 0.002030 Acc: 0.84% Precision: 0.27 Recall: 0.34 F1: 0.30\n",
      "Best loss model saved!\n",
      "Epoch [30/1000]  Train Loss: 0.001458 Acc: 0.860884 | Val Loss: 0.002027 Acc: 0.84% Precision: 0.28 Recall: 0.36 F1: 0.31\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [31/1000]  Train Loss: 0.001443 Acc: 0.858027 | Val Loss: 0.002024 Acc: 0.84% Precision: 0.27 Recall: 0.36 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [32/1000]  Train Loss: 0.001441 Acc: 0.857823 | Val Loss: 0.002021 Acc: 0.83% Precision: 0.26 Recall: 0.36 F1: 0.30\n",
      "Best loss model saved!\n",
      "Epoch [33/1000]  Train Loss: 0.001447 Acc: 0.859184 | Val Loss: 0.002024 Acc: 0.84% Precision: 0.28 Recall: 0.34 F1: 0.31\n",
      "Epoch [34/1000]  Train Loss: 0.001434 Acc: 0.860408 | Val Loss: 0.002017 Acc: 0.83% Precision: 0.27 Recall: 0.38 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [35/1000]  Train Loss: 0.001421 Acc: 0.857619 | Val Loss: 0.002017 Acc: 0.83% Precision: 0.27 Recall: 0.38 F1: 0.32\n",
      "Best f1 model saved!\n",
      "Epoch [36/1000]  Train Loss: 0.001420 Acc: 0.856190 | Val Loss: 0.002012 Acc: 0.83% Precision: 0.26 Recall: 0.38 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [37/1000]  Train Loss: 0.001411 Acc: 0.856599 | Val Loss: 0.002011 Acc: 0.83% Precision: 0.27 Recall: 0.38 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [38/1000]  Train Loss: 0.001421 Acc: 0.858163 | Val Loss: 0.002011 Acc: 0.83% Precision: 0.26 Recall: 0.38 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [39/1000]  Train Loss: 0.001412 Acc: 0.855238 | Val Loss: 0.002012 Acc: 0.83% Precision: 0.27 Recall: 0.38 F1: 0.32\n",
      "Epoch [40/1000]  Train Loss: 0.001405 Acc: 0.856667 | Val Loss: 0.002012 Acc: 0.83% Precision: 0.27 Recall: 0.38 F1: 0.32\n",
      "Epoch [41/1000]  Train Loss: 0.001396 Acc: 0.856667 | Val Loss: 0.002008 Acc: 0.82% Precision: 0.26 Recall: 0.40 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [42/1000]  Train Loss: 0.001401 Acc: 0.852653 | Val Loss: 0.002008 Acc: 0.83% Precision: 0.26 Recall: 0.40 F1: 0.32\n",
      "Epoch [43/1000]  Train Loss: 0.001397 Acc: 0.851905 | Val Loss: 0.002007 Acc: 0.82% Precision: 0.25 Recall: 0.40 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [44/1000]  Train Loss: 0.001398 Acc: 0.852721 | Val Loss: 0.002006 Acc: 0.83% Precision: 0.26 Recall: 0.40 F1: 0.32\n",
      "Best loss model saved!\n",
      "Best f1 model saved!\n",
      "Epoch [45/1000]  Train Loss: 0.001388 Acc: 0.853197 | Val Loss: 0.002005 Acc: 0.82% Precision: 0.25 Recall: 0.40 F1: 0.31\n",
      "Best loss model saved!\n",
      "Epoch [46/1000]  Train Loss: 0.001387 Acc: 0.851565 | Val Loss: 0.002011 Acc: 0.83% Precision: 0.27 Recall: 0.38 F1: 0.31\n",
      "Epoch [47/1000]  Train Loss: 0.001375 Acc: 0.853469 | Val Loss: 0.002008 Acc: 0.82% Precision: 0.26 Recall: 0.40 F1: 0.31\n",
      "Epoch [48/1000]  Train Loss: 0.001376 Acc: 0.852449 | Val Loss: 0.002008 Acc: 0.83% Precision: 0.26 Recall: 0.40 F1: 0.32\n",
      "Epoch [49/1000]  Train Loss: 0.001375 Acc: 0.850136 | Val Loss: 0.002007 Acc: 0.83% Precision: 0.26 Recall: 0.40 F1: 0.32\n",
      "Epoch [50/1000]  Train Loss: 0.001368 Acc: 0.854014 | Val Loss: 0.002007 Acc: 0.83% Precision: 0.26 Recall: 0.38 F1: 0.31\n",
      "Epoch [51/1000]  Train Loss: 0.001360 Acc: 0.853265 | Val Loss: 0.002008 Acc: 0.83% Precision: 0.26 Recall: 0.38 F1: 0.31\n",
      "Epoch [52/1000]  Train Loss: 0.001374 Acc: 0.848776 | Val Loss: 0.002007 Acc: 0.83% Precision: 0.27 Recall: 0.41 F1: 0.33\n",
      "Best f1 model saved!\n",
      "Epoch [53/1000]  Train Loss: 0.001357 Acc: 0.851973 | Val Loss: 0.002010 Acc: 0.83% Precision: 0.26 Recall: 0.38 F1: 0.31\n",
      "Epoch [54/1000]  Train Loss: 0.001378 Acc: 0.850544 | Val Loss: 0.002008 Acc: 0.83% Precision: 0.27 Recall: 0.41 F1: 0.32\n"
     ]
    }
   ],
   "source": [
    "trainingData = train_model(Ivan_NN, nInputs, train_loader, val_loader, epoch, learningRate, treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"loss_model.pkl\", \"rb\") as f:\n",
    "    bestModelData = pickle.load(f)\n",
    "    print(\"File loaded\") \n",
    "\n",
    "lossModelData = train_loss_model(Ivan_NN, nInputs,bestModelData, trainVal_loader, test_loader, epoch, learningRate=learningRate, treshold=treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1ModelData = train_f1_model(Ivan_NN, nInputs,trainingData[\"best initial f1 model\"], trainVal_loader, test_loader, epoch, learningRate=learningRate, treshold=treshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
