{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37da2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "class Ivan_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, 2180)\n",
    "        self.input_bn = nn.BatchNorm1d(2180)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(2180, 888),\n",
    "            nn.BatchNorm1d(888),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(888, 1122),\n",
    "            nn.BatchNorm1d(1122),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(1122, 624),\n",
    "            nn.BatchNorm1d(624),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(624, 1080),\n",
    "            nn.BatchNorm1d(1080),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(1080, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),  # ← Fixed this\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Linear(128, 36),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Linear(36, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),  # or LeakyReLU, up to you\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Skip connection projectors\n",
    "        self.skip1_proj = nn.Sequential(nn.Linear(2180, 1122))\n",
    "        self.skip2_proj = nn.Sequential(nn.Linear(1122, 128))\n",
    "        self.skip3_proj = nn.Sequential(nn.Linear(128, 64))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.input_bn(x)\n",
    "        x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "\n",
    "        skip1 = self.skip1_proj(x)\n",
    "        x2 = x2 + skip1\n",
    "\n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        x5 = self.block5(x4)\n",
    "\n",
    "        x6 = self.block6(x5)\n",
    "\n",
    "        skip2 = self.skip2_proj(x2)\n",
    "        x6 = x6 + skip2\n",
    "\n",
    "        x7 = self.block7(x6)\n",
    "        x8 = self.block8(x7)\n",
    "\n",
    "        skip3 = self.skip3_proj(x6)\n",
    "        x8 = x8 + skip3\n",
    "\n",
    "        out = self.output(x8)\n",
    "        return out\n",
    "\n",
    "\n",
    "def cleanRawData(filename:str = \"./data/raw_data.csv\", colsToDrop:list[str] = [\"name\", \"DR1\", \"DR2\", \"image1\", \"image2\", \"CSME1\", \"HBPRP1\", \"CSME2\", \"HBPRP2\", \"WorseDR\", \"ACR\"]) -> pd.DataFrame:\n",
    "    rawData = pd.read_csv(filename)\n",
    "    rawData1 = rawData.dropna()\n",
    "    rawData2 = rawData1.drop(columns=colsToDrop)\n",
    "    #? Change gender to binary instead of 1s and 2s\n",
    "    rawData2[\"gender\"] = rawData2[\"gender\"].replace(2,0).astype(float)\n",
    "    #? Hot-encode Categorical column\n",
    "    community = pd.get_dummies(rawData2[\"community\"],).astype(float)\n",
    "    #? remove original categorical column and join Hotencoded columns\n",
    "    notSoRawData = pd.concat([rawData2.drop(columns=[\"community\"]),community], axis=1)\n",
    "    for col in notSoRawData.columns.tolist():\n",
    "        if notSoRawData[col].dtype == 'object':\n",
    "            notSoRawData[col] = notSoRawData[col].str.replace(',', '').astype(float)\n",
    "    return notSoRawData[~notSoRawData.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.DataFrame):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "def train_model(model:Ivan_NN, nFeatures:int, train_loader, val_loader, epochs=10, learningRate = 0.0001, treshold:float = 0.5, patience = 30, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    theMODEL = model(nFeatures)\n",
    "    theMODEL.to(device)\n",
    "    print(theMODEL)\n",
    "\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(theMODEL.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies= []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_f1s = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_f1 = .0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        theMODEL.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = theMODEL(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        theMODEL.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "                outputs = theMODEL(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() # * inputs.size(0)\n",
    "\n",
    "                if pd.isna(loss.item()):\n",
    "                    print( \"Val Loss:\" ,loss)\n",
    "                    in_rows = torch.isnan(inputs).any(axis=1)\n",
    "                    out_rows = torch.isnan(outputs).any(axis=-1)\n",
    "                    targets_rows = torch.isnan(targets).any(axis=-1)\n",
    "                    print(inputs[in_rows], outputs[out_rows], targets[targets_rows],sep=\"\\n\")\n",
    "                    return\n",
    "                \n",
    "                # preds = torch.round(torch.sigmoid(outputs))\n",
    "                preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "                val_correct += (preds == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "                f1Score = f1_score(targets, preds)\n",
    "                precision = precision_score(targets,preds,zero_division=.0)\n",
    "                recall = recall_score(targets, preds)\n",
    "                accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "\n",
    "        if val_total == 0:\n",
    "            print(\"Empty validation set!\")\n",
    "            return\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "        val_f1s.append(f1Score)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \"\n",
    "            #   f\"Val Loss: {val_loss:.6f} Acc: {val_acc:.6f}\"\n",
    "              f\"Val Loss: {val_loss:.6f} Acc: {accuracy:.2f}% Precision: {precision:.2f} Recall: {recall:.2f} F1: {f1Score:.2f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_loss_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best loss model saved!\")\n",
    "        if f1Score > best_val_f1:\n",
    "            best_val_f1 = f1Score\n",
    "            best_f1_model = {\n",
    "                \"epoch\": epoch+1,\n",
    "                \"stateDict\": theMODEL.state_dict(),\n",
    "            }\n",
    "            print(\"Best f1 model saved!\")\n",
    "        elif val_loss > best_val_loss and epoch > best_loss_model[\"epoch\"] + patience:\n",
    "            print(f\"Early stopping... Current Val Loss:{val_loss}  vs  Best Val Loss: {best_val_loss}\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='x')\n",
    "    plt.title(\"Training vs. Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.plot(val_accuracies, label='Val Acc', marker='x')\n",
    "    plt.title(\"Training vs. Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_precisions, label='Val Precision', marker='^')\n",
    "    plt.plot(val_recalls, label='Val Recall', marker='x')\n",
    "    plt.plot(val_f1s, label='Val F1', marker='|')\n",
    "    plt.title(\"Precision, Recall, F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    trainingData = {\n",
    "        \"learningRate\": learningRate,\n",
    "        \"epochs\": (epoch, epochs),\n",
    "        \"treshold\": treshold,\n",
    "        \"initial training loss\": train_losses,\n",
    "        \"initial training acc\": train_accuracies,\n",
    "        \"initial validation loss\": val_losses,\n",
    "        \"initial validation acc\": val_accuracies,\n",
    "        \"initial validation precision\": val_precisions,\n",
    "        \"initial validation recall\": val_recalls,\n",
    "        \"initial validation F1\":val_f1s, \n",
    "        \"best initial loss model\": best_loss_model,\n",
    "        \"best initial f1 model\": best_f1_model,\n",
    "    }\n",
    "    return trainingData\n",
    "\n",
    "    \n",
    "def train_loss_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, trainVal_loader, test_loader, epochs=20, learningRate = 0.00001, treshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Loading best loss model State from epoch {bestModelData['epoch']}\")\n",
    "    finalModel = model(nFeatures)\n",
    "    finalModel.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(finalModel.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData[\"epoch\"]):\n",
    "        finalModel.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in trainVal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    finalModel.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = finalModel(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            # preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    loss_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": finalModel.state_dict(),\n",
    "    }\n",
    "\n",
    "\n",
    "    return loss_Model\n",
    "\n",
    "def train_f1_model(model:Ivan_NN, nFeatures:int, bestModelData:dict, trainVal_loader, test_loader, epochs=20, learningRate = 0.00001, treshold:float = 0.5, device=None):\n",
    "    # Use GPU if available\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Loading best f1 model State from epoch {bestModelData['epoch']}\")\n",
    "    f1Model = model(nFeatures)\n",
    "    f1Model.load_state_dict(bestModelData[\"stateDict\"])\n",
    "    posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "    optimizer = optim.Adam(f1Model.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(bestModelData['epoch']):\n",
    "        f1Model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in trainVal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()# * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\".ljust(16),\n",
    "              f\"Train Loss: {train_loss:.6f} Acc: {train_acc:.6f} | \")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "\n",
    "    f1Model.eval()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
    "            outputs = f1Model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss = loss.item() # * inputs.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            preds = (torch.sigmoid(outputs) >= treshold).float()\n",
    "\n",
    "            test_f1Score = f1_score(targets, preds)\n",
    "            test_precision = precision_score(targets,preds)\n",
    "            test_recall = recall_score(targets, preds)\n",
    "            test_accuracy = accuracy_score(targets,preds)\n",
    "\n",
    "            qwerty = [\n",
    "                f\"Loss: {test_loss:.6f}\", \n",
    "                f\"Accuracy: {test_accuracy:.2f}%\", \n",
    "                f\"Precision: {test_precision:.2f}\", \n",
    "                f\"Recall: {test_recall:.2f}\", \n",
    "                f\"F1 Score: {test_f1Score:.2f}\",\n",
    "            ]\n",
    "            wwidth = 30\n",
    "            print(\"Test Results\".center(16).center(wwidth,\"=\"))\n",
    "            for line in qwerty:\n",
    "                print(line.ljust(16).center(wwidth-2).center(wwidth,\"|\"))\n",
    "            # print(\"Test\", f\"Loss: {test_loss:.6f}\", f\"Accuracy: {test_accuracy:.2f}%\", f\"Precision: {test_precision:.2f}\", f\"Recall: {test_recall:.2f}\", f\"F1 Score: {test_f1Score:.2f}%\", sep=\"\\n \")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='^')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Acc', marker='^')\n",
    "    plt.title(\"Training Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    f1_Model = {\n",
    "        \"final training loss\": train_losses,\n",
    "        \"final training acc\": train_accuracies,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Test acc\": test_accuracy,\n",
    "        \"Test precision\": test_precision,\n",
    "        \"Test recall\": test_recall,\n",
    "        \"Test F1\": test_f1Score,\n",
    "        \"final model\": f1Model.state_dict(),\n",
    "    }\n",
    "\n",
    "    \n",
    "    return f1_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2560759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory vanData created\n",
      "Original class distribution: DR\n",
      "0.0    4128\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    4128\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\SUTD\\Term 6\\Applied Deep Learning\\Project\\ADL2\\venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -21.132: 100%|██████████| 10000/10000 [1:51:57<00:00,  1.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 8989/8989 [00:01<00:00, 6854.42it/s]\n",
      "Sampling conditions: 100%|██████████| 1010/1010 [00:04<00:00, 248.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    8989\n",
      "1.0    1010\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 20/20 [00:06<00:00,  3.25it/s]|\n",
      "Column Shapes Score: 90.2%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 190/190 [00:02<00:00, 84.53it/s]|\n",
      "Column Pair Trends Score: 88.31%\n",
      "\n",
      "Overall Score (Average): 89.26%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    13117\n",
      "1.0     1474\n",
      "Name: count, dtype: int64\n",
      "Saving generated training and validation datasets\n",
      "Original class distribution: DR\n",
      "0.0    4645\n",
      "1.0     522\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    4645\n",
      "1.0     522\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\SUTD\\Term 6\\Applied Deep Learning\\Project\\ADL2\\venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -18.167:  69%|██████▉   | 6925/10000 [1:27:48<46:57,  1.09it/s]  "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Helper_Functions import *\n",
    "from Preprocessing_Functions2 import * \n",
    "path = \"../ADL2/DATA_21/training_set/training_data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# data = cleanRawData()\n",
    "X , Y = data.drop(columns=[\"DR\"]).astype(float), data[[\"DR\"]].astype(float)\n",
    "\n",
    "trainVal, testing = train_test_split(data, test_size=0.1, random_state=42, stratify=data[\"DR\"])\n",
    "training, validation = train_test_split(trainVal, test_size=1/9, random_state=42, stratify=trainVal[\"DR\"])\n",
    "\n",
    "# trainVal_X, test_X, trainVal_Y, test_Y = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "# train_X, val_X, train_Y, val_Y = train_test_split(trainVal_X, trainVal_Y, test_size=1/9, random_state=42, stratify=trainVal_Y)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "scaler = StandardScaler()\n",
    "cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', \n",
    "             'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "cat_cols = ['Gender', 'Community'] \n",
    "y_col = 'DR'\n",
    "\n",
    "# Create the directory\n",
    "directory = \"vanData\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "print(f\"directory {directory} created\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#* OUTLIER DETECTION\n",
    "X_train_processed = Outlier_Removal(training, \n",
    "                                    OD_majority=None,\n",
    "                                    OD_minority=None,\n",
    "                                    )\n",
    "# X_train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "# #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "X_train_processed = Synthetic_Data_Generator2(X_train_processed, \"\", synthesizer=\"TVAE\", epochs=10000, batch_size=700, n_synthetic_data=10000)\n",
    "\n",
    "print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "#* Calculate BMI, TCTG & ENCODING\n",
    "X_train_processed, validation = get_bmi(X_train_processed, validation)\n",
    "X_train_processed, validation = get_TCTG(X_train_processed, validation)\n",
    "X_train_processed, validation = apply_one_hot_encoding(X_train_processed, validation)\n",
    "#* Scaler\n",
    "X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "validation[cont_cols] = scaler.transform(validation[cont_cols])\n",
    "\n",
    "print(\"Saving generated training and validation datasets\")\n",
    "X_train_processed.to_csv(f\"{directory}/training.csv\")\n",
    "validation.to_csv(f\"{directory}/validation.csv\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# #* OUTLIER DETECTION\n",
    "fullTrain_processed = Outlier_Removal(trainVal, \n",
    "                                    OD_majority=None,\n",
    "                                    OD_minority=None,\n",
    "                                    )\n",
    "# X_train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "# #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "print(\"Before oversampling & synthetic data:\", fullTrain_processed[[\"DR\"]].value_counts())\n",
    "fullTrain_processed = Synthetic_Data_Generator2(fullTrain_processed, \"\", synthesizer=\"TVAE\", epochs=10000, batch_size=700, n_synthetic_data=10000)\n",
    "\n",
    "print(\"After oversampling & synthetic data:\", fullTrain_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "#* Calculate BMI, TCTG & ENCODING\n",
    "fullTrain_processed, testing = get_bmi(fullTrain_processed, testing)\n",
    "fullTrain_processed, testing = get_TCTG(fullTrain_processed, testing)\n",
    "fullTrain_processed, testing = apply_one_hot_encoding(fullTrain_processed, testing)\n",
    "#* Scaler\n",
    "fullTrain_processed[cont_cols] = scaler.fit_transform(fullTrain_processed[cont_cols])\n",
    "testing[cont_cols] = scaler.transform(testing[cont_cols])\n",
    "\n",
    "print(\"Saving generated combinedTrain and test datasets\")\n",
    "fullTrain_processed.to_csv(f\"{directory}/fullTrain.csv\")\n",
    "testing.to_csv(f\"{directory}/testing.csv\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#? For Training and Cross Validation Loop\n",
    "train_X_n, train_Y = fullTrain_processed.drop(columns=['DR']), fullTrain_processed[['DR']]\n",
    "# train_X_n = (train_X - train_X.min()) / (train_X.max() - train_X.min())\n",
    "train_dataset = TabularDataset(train_X_n, train_Y)\n",
    "\n",
    "val_X_n , val_Y = validation.drop(columns=['DR']), validation[['DR']]\n",
    "# val_X_n = (val_X - val_X.min()) / (val_X.max() - val_X.min())\n",
    "val_dataset = TabularDataset(val_X_n, val_Y)\n",
    "\n",
    "#? For Final Training and Testing Loop\n",
    "trainVal_X_n , trainVal_Y = fullTrain_processed.drop(columns=['DR']), fullTrain_processed[['DR']]\n",
    "# trainVal_X_n = (trainVal_X - trainVal_X.min()) / (trainVal_X.max() - trainVal_X.min())\n",
    "trainVal_dataset = TabularDataset(trainVal_X_n, trainVal_Y)\n",
    "\n",
    "test_X_n , test_Y = testing.drop(columns=['DR']), testing[['DR']]\n",
    "# test_X_n = (test_X - test_X.min()) / (test_X.max() - test_X.min())\n",
    "test_dataset = TabularDataset(test_X_n, test_Y)\n",
    "\n",
    "nInputs = len(X.columns)\n",
    "batchSize = 700\n",
    "epoch = 1000\n",
    "learningRate = 0.000001\n",
    "treshold = 0.65\n",
    "# model = Ivan_NN2(nInputs)\n",
    "\n",
    "#? Attemps to \"Balance\" the class imbalance\n",
    "# posWeight = torch.tensor(10, dtype=torch.float32).to(\"cpu\")\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
    "# optimiser = optim.Adam(model.parameters() ,lr=learningRate, weight_decay=1e-4)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, drop_last=True)\n",
    "trainVal_loader = DataLoader(trainVal_dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = train_model(Ivan_NN, nInputs, train_loader, val_loader, epoch, learningRate, treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossModelData = train_loss_model(Ivan_NN, nInputs,trainingData[\"best initial loss model\"], trainVal_loader, test_loader, epoch, learningRate=learningRate, treshold=treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1ModelData = train_f1_model(Ivan_NN, nInputs,trainingData[\"best initial f1 model\"], trainVal_loader, test_loader, epoch, learningRate=learningRate, treshold=treshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
