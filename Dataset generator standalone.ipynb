{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749996a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAN LE ZHAN\\Documents\\GitHub\\ADL2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Training_Helper_Functions import *\n",
    "from Preprocessing_Functions import * \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42 \n",
    "raw_dataset = pd.read_csv(\"./original_dataset/processed_data_encoded.csv\") #data has X and Y, community 0-9\n",
    "# X = raw_dataset.drop(columns=[\"BMI\", \"TCTG\", \"DR\"])\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)\n",
    "df = pd.concat([X_FOR_FOLDS, Y_FOR_FOLDS], axis=1)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(\"./DATA/training_set/training_data_for_folds.csv\", index=False)\n",
    "df_test = pd.concat([X_FINAL_TEST, Y_FINAL_TEST], axis=1)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_test.to_csv(\"./DATA/holdout_set/holdout_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3fc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "import sdv\n",
    "from sdv.metadata import Metadata\n",
    "\n",
    "def get_bmi_i(df):\n",
    "    # Calculate BMI for both training and test sets\n",
    "    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_TCTG_i(df):\n",
    "    # Calculate TCTG for both training and test sets\n",
    "    df['TCTG'] = df['TC'] / df['TG']\n",
    "    return df\n",
    "def Synthetic_Data_Generator(df_train, synthesizer = \"TVAE\", epochs = 200, batch_size = 128, n_synthetic_data = 1000): \n",
    "    \"\"\"Conditions: \"balanced\" or None\"\"\"\n",
    "    df_train = df_train.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "    metadata = Metadata.detect_from_dataframe(data=df_train)\n",
    "    metadata.validate()\n",
    "    \n",
    "    #* Synthetic Data generation conditions\n",
    "    condition_list = []\n",
    "    #* Synthesizer setup\n",
    "    if synthesizer == \"CTGAN\":\n",
    "        filepath = f\"{synthesizer}_{epochs}.pkl\"\n",
    "        synthesizer = CTGANSynthesizer(\n",
    "                                metadata=metadata, \n",
    "                                enforce_min_max_values=True, \n",
    "                                enforce_rounding=True, \n",
    "                                epochs = epochs,\n",
    "                                verbose=True, \n",
    "                                cuda=True,\n",
    "                                batch_size=300 # need to be divisible by 10 or pac size\n",
    "                                )  \n",
    "        # df_train = make_divisible(df_train, 10)\n",
    "    elif synthesizer == \"TVAE\":\n",
    "        filepath = f\"{synthesizer}_{epochs}.pkl\"\n",
    "        synthesizer = TVAESynthesizer(\n",
    "                                metadata=metadata, \n",
    "                                enforce_min_max_values=True, \n",
    "                                enforce_rounding=True, \n",
    "                                epochs = epochs,\n",
    "                                verbose=True, \n",
    "                                cuda=True,\n",
    "                                batch_size=batch_size,\n",
    "                                )\n",
    "    else:\n",
    "        return df_train\n",
    "    print(\"Balancing condition applied\")\n",
    "    \n",
    "    # Step 1: Fit the synthesizer\n",
    "    synthesizer.fit(df_train)\n",
    "    \n",
    "    synthesizer.save(filepath)\n",
    "    # Step 2: Get class counts\n",
    "    # Step 1: Get class counts\n",
    "    count_0 = df_train[df_train['DR'] == 0].shape[0]\n",
    "    count_1 = df_train[df_train['DR'] == 1].shape[0]\n",
    "\n",
    "    # Step 2: Balance to the max count\n",
    "    balanced_per_class = max(count_0, count_1)\n",
    "\n",
    "    cond_0 = Condition(column_values={'DR': 0}, num_rows=balanced_per_class - count_0)\n",
    "    cond_1 = Condition(column_values={'DR': 1}, num_rows=balanced_per_class - count_1)\n",
    "\n",
    "    balanced_data = synthesizer.sample_from_conditions([cond_0, cond_1])\n",
    "\n",
    "    # Step 3: Add more *evenly* on top to hit n_synthetic_data\n",
    "    # Note: You already have (balanced_per_class * 2) at this point\n",
    "\n",
    "    current_total = balanced_per_class * 2\n",
    "    remaining = n_synthetic_data - current_total\n",
    "\n",
    "    # Split remaining evenly across classes\n",
    "    extra_per_class = remaining // 2\n",
    "\n",
    "    # (optional: +1 to one class if remaining is odd)\n",
    "    cond_extra_0 = Condition(column_values={'DR': 0}, num_rows=extra_per_class)\n",
    "    cond_extra_1 = Condition(column_values={'DR': 1}, num_rows=remaining - extra_per_class)\n",
    "\n",
    "    extra_data = synthesizer.sample_from_conditions([cond_extra_0, cond_extra_1])\n",
    "\n",
    "    # Step 5: Combine all the synthetic garbage\n",
    "    synthetic_data = pd.concat([balanced_data, extra_data], ignore_index=True)\n",
    "    quality_report = evaluate_quality(df_train, synthetic_data, metadata)\n",
    "    synthetic_data = get_bmi_i(synthetic_data)\n",
    "    synthetic_data = get_TCTG_i(synthetic_data)\n",
    "    \n",
    "    # Ensure folder exists\n",
    "\n",
    "    # Save to specific file\n",
    "    \n",
    "    synthetic_data.to_csv(f\"./DATA/synthetic_training_set/synthetic_data_{epochs}_TVAE.csv\", index=False)\n",
    "\n",
    "    # synthetic_data.to_csv('./synthetic_dataset/synthetic_data2.csv', index=False)\n",
    "    df_train = pd.concat([synthetic_data, df_train], ignore_index=True)\n",
    "    return df_train\n",
    "\n",
    "def FOLDS_GENERATOR_Synthetic(dataset, n_splits=5, random_state=42, \n",
    "                    OD_majority=None, OD_minority=None,\n",
    "                    synthesizer = \"TVAE\", epochs = 200, n_synthetic_data=0, \n",
    "                    scaler=None):\n",
    "    \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    kFolds_list = []\n",
    "\n",
    "    # Convert column names to strings to ensure compatibility\n",
    "    df = dataset.copy()\n",
    "    X = df.drop(columns=[\"DR\"])\n",
    "    Y = pd.DataFrame(df[\"DR\"])\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        train = pd.concat([X.iloc[train_idx], Y.iloc[train_idx]], axis=1)\n",
    "        test = pd.concat([X.iloc[test_idx], Y.iloc[test_idx]], axis=1)\n",
    "        \n",
    "        #* OUTLIER DETECTION\n",
    "        X_train_processed = Outlier_Removal(train, \n",
    "                                            OD_majority=OD_majority,\n",
    "                                            OD_minority=OD_minority,\n",
    "                                            )\n",
    "        \n",
    "        #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "        print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        X_train_processed = Synthetic_Data_Generator(X_train_processed, synthesizer=synthesizer, epochs=epochs, batch_size=512, n_synthetic_data=n_synthetic_data)\n",
    "            \n",
    "        print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        \n",
    "        #* Calculate BMI & ENCODING\n",
    "        # X_train_processed, test = get_bmi(X_train_processed, test)\n",
    "        # X_train_processed, test = get_TCTG(X_train_processed, test)\n",
    "        X_train_processed, test = apply_one_hot_encoding(X_train_processed, test)\n",
    "        #* Scaler\n",
    "        X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "        test[cont_cols] = scaler.transform(test[cont_cols])\n",
    "        # Append processed data (excluding the target column 'DR')\n",
    "        \n",
    "        \n",
    "        kFolds_list.append((\n",
    "                            X_train_processed.drop(columns=['DR']),\n",
    "                            test.drop(columns=['DR']),\n",
    "                            X_train_processed['DR'].values.reshape(-1, 1),  # Ensures the target is 2D\n",
    "                            test['DR'].values.reshape(-1, 1)  # Ensures the target is 2D\n",
    "                        ))\n",
    "        break\n",
    "    print(f\"Fold: {fold+1}, Train: {X_train_processed.drop(columns=['DR']).shape}, Test: {test.drop(columns=['DR']).shape}\")\n",
    "    return kFolds_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9429ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: DR\n",
      "0.0    4129\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "After OD, majority: 1911\n",
      "After OD, minority: 210\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    1911\n",
      "1.0     210\n",
      "Name: count, dtype: int64\n",
      "Balancing condition applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAN LE ZHAN\\Documents\\GitHub\\ADL2\\.venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: 2.715: 100%|██████████| 1000/1000 [01:49<00:00,  9.13it/s]\n",
      "Sampling conditions: 100%|██████████| 1701/1701 [00:00<00:00, 3796.88it/s]\n",
      "Sampling conditions: 100%|██████████| 6178/6178 [00:00<00:00, 8761.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 18/18 [00:00<00:00, 59.56it/s]|\n",
      "Column Shapes Score: 84.38%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 153/153 [00:00<00:00, 347.19it/s]|\n",
      "Column Pair Trends Score: 86.75%\n",
      "\n",
      "Overall Score (Average): 85.56%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (10000, 28), Test: (1149, 28)\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "kFolds = FOLDS_GENERATOR_Synthetic(df, n_splits=5, random_state=42,             \n",
    "                            # OD_majority = IsolationForest(contamination=trial.suggest_float(\"contamination_majority\", 0.01, 0.4), random_state=random_state, \n",
    "                            OD_majority = IQRDetector(factor=1),\n",
    "                            OD_minority = IQRDetector(factor=1),\n",
    "         \n",
    "                            synthesizer = \"TVAE\",\n",
    "                            epochs = 1000,\n",
    "                            n_synthetic_data = 10000,\n",
    "                            scaler=scaler,      \n",
    "                            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc820c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Community</th>\n",
       "      <th>UAlb</th>\n",
       "      <th>Ucr</th>\n",
       "      <th>UACR</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>LDLC</th>\n",
       "      <th>HDLC</th>\n",
       "      <th>Scr</th>\n",
       "      <th>BUN</th>\n",
       "      <th>FPG</th>\n",
       "      <th>HbA1c</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>DR</th>\n",
       "      <th>BMI</th>\n",
       "      <th>TCTG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.61440</td>\n",
       "      <td>0.59310</td>\n",
       "      <td>4.09210</td>\n",
       "      <td>13.710960</td>\n",
       "      <td>3713.262200</td>\n",
       "      <td>14.005390</td>\n",
       "      <td>5.159593</td>\n",
       "      <td>1.371252</td>\n",
       "      <td>3.222766</td>\n",
       "      <td>1.321891</td>\n",
       "      <td>58.420100</td>\n",
       "      <td>5.564700</td>\n",
       "      <td>8.76655</td>\n",
       "      <td>7.195940</td>\n",
       "      <td>161.057500</td>\n",
       "      <td>62.703390</td>\n",
       "      <td>7.518830</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>24.124111</td>\n",
       "      <td>4.566567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.88734</td>\n",
       "      <td>0.49128</td>\n",
       "      <td>2.94236</td>\n",
       "      <td>11.472183</td>\n",
       "      <td>5160.705713</td>\n",
       "      <td>12.222573</td>\n",
       "      <td>0.702019</td>\n",
       "      <td>0.653386</td>\n",
       "      <td>0.657130</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>13.262753</td>\n",
       "      <td>1.221509</td>\n",
       "      <td>2.23005</td>\n",
       "      <td>1.284794</td>\n",
       "      <td>6.806881</td>\n",
       "      <td>7.465654</td>\n",
       "      <td>5.718972</td>\n",
       "      <td>0.420532</td>\n",
       "      <td>1.956432</td>\n",
       "      <td>1.955758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.40000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.992187</td>\n",
       "      <td>1.074194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>4.670000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.851562</td>\n",
       "      <td>2.919511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>5.140000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>8.50000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.074668</td>\n",
       "      <td>4.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>68.25000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7674.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.630000</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>3.680000</td>\n",
       "      <td>1.510000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>10.20000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.390320</td>\n",
       "      <td>5.866837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>102.700000</td>\n",
       "      <td>19307.000000</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.990000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>15.70000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.965661</td>\n",
       "      <td>13.925000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age       Gender    Community          UAlb           Ucr  \\\n",
       "count  10000.00000  10000.00000  10000.00000  10000.000000  10000.000000   \n",
       "mean      63.61440      0.59310      4.09210     13.710960   3713.262200   \n",
       "std        6.88734      0.49128      2.94236     11.472183   5160.705713   \n",
       "min       46.00000      0.00000      0.00000      0.100000      1.000000   \n",
       "25%       59.00000      0.00000      2.00000      6.600000      1.000000   \n",
       "50%       64.00000      1.00000      4.00000     10.400000     13.000000   \n",
       "75%       68.25000      1.00000      7.00000     16.700000   7674.000000   \n",
       "max       79.00000      1.00000      9.00000    102.700000  19307.000000   \n",
       "\n",
       "               UACR            TC            TG          LDLC          HDLC  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      14.005390      5.159593      1.371252      3.222766      1.321891   \n",
       "std       12.222573      0.702019      0.653386      0.657130      0.249000   \n",
       "min        0.100000      3.260000      0.400000      1.330000      0.730000   \n",
       "25%        5.900000      4.670000      0.870000      2.760000      1.130000   \n",
       "50%        9.800000      5.140000      1.150000      3.220000      1.290000   \n",
       "75%       18.000000      5.630000      1.780000      3.680000      1.510000   \n",
       "max      118.700000      7.370000      3.300000      5.100000      1.990000   \n",
       "\n",
       "                Scr           BUN          FPG         HbA1c        Height  \\\n",
       "count  10000.000000  10000.000000  10000.00000  10000.000000  10000.000000   \n",
       "mean      58.420100      5.564700      8.76655      7.195940    161.057500   \n",
       "std       13.262753      1.221509      2.23005      1.284794      6.806881   \n",
       "min       30.000000      3.000000      4.40000      4.400000    144.000000   \n",
       "25%       48.000000      4.600000      7.00000      6.100000    157.000000   \n",
       "50%       56.000000      5.500000      8.50000      7.000000    160.000000   \n",
       "75%       68.000000      6.400000     10.20000      8.200000    167.000000   \n",
       "max       90.000000      9.200000     15.70000     11.000000    180.000000   \n",
       "\n",
       "             Weight      Duration            DR           BMI          TCTG  \n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
       "mean      62.703390      7.518830      0.229500     24.124111      4.566567  \n",
       "std        7.465654      5.718972      0.420532      1.956432      1.955758  \n",
       "min       40.000000      0.100000      0.000000     16.992187      1.074194  \n",
       "25%       57.500000      2.100000      0.000000     22.851562      2.919511  \n",
       "50%       61.800000      6.500000      0.000000     24.074668      4.405405  \n",
       "75%       67.700000     12.300000      0.000000     25.390320      5.866837  \n",
       "max       83.000000     25.000000      1.000000     32.965661     13.925000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./DATA/synthetic_training_set/synthetic_data_2000_TVAE.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cea845",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msdv\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msdv\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Metadata\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m synthetic_data = \u001b[43mread_csv\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m./synthetic_dataset/synthetic_data_only.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m metadata = Metadata.detect_from_dataframe(data=real_data)\n\u001b[32m      8\u001b[39m metadata.validate()\n",
      "\u001b[31mNameError\u001b[39m: name 'read_csv' is not defined"
     ]
    }
   ],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "import sdv\n",
    "from sdv.metadata import Metadata\n",
    "\n",
    "synthetic_data = read_csv(\"./synthetic_dataset/synthetic_data_only.csv\")\n",
    "metadata = Metadata.detect_from_dataframe(data=real_data)\n",
    "metadata.validate()\n",
    "metadata.visualize()\n",
    "# 1. perform basic validity checks\n",
    "diagnostic = run_diagnostic(df, synthetic_data, metadata)\n",
    "\n",
    "# 2. measure the statistical similarity\n",
    "quality_report = evaluate_quality(real_data, synthetic_data, metadata)\n",
    "\n",
    "# # 3. plot the data\n",
    "# fig = get_column_plot(\n",
    "#     real_data=real_data,\n",
    "#     synthetic_data=synthetic_data,\n",
    "#     metadata=metadata,\n",
    "#     column_name='Age' #change this u decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e767b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
