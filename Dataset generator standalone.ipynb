{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd18b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTraining_Helper_Functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPreprocessing_Functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustScaler\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\Preprocessing_Functions.py:90\u001b[39m\n\u001b[32m     87\u001b[39m     df_train = pd.concat([synthetic_data, df_train], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_train\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTENC\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_smotenc_oversampling\u001b[39m(df_train):\n\u001b[32m     93\u001b[39m     cont_cols = [\u001b[33m'\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUAlb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUcr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUACR\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTC\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTG\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTCTG\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     94\u001b[39m                  \u001b[33m'\u001b[39m\u001b[33mLDLC\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHDLC\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mScr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBUN\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFPG\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHbA1c\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHeight\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mWeight\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBMI\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDuration\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from Training_Helper_Functions import *\n",
    "from Preprocessing_Functions import * \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749996a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAN LE ZHAN\\Documents\\GitHub\\ADL2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "random_state = 42 \n",
    "raw_dataset = pd.read_csv(\"./original_dataset/processed_data_encoded.csv\") #data has X and Y, community 0-9\n",
    "# X = raw_dataset.drop(columns=[\"BMI\", \"TCTG\", \"DR\"])\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)\n",
    "df = pd.concat([X_FOR_FOLDS, Y_FOR_FOLDS], axis=1)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(\"./DATA/training_set/training_data_for_folds.csv\", index=False)\n",
    "df_test = pd.concat([X_FINAL_TEST, Y_FINAL_TEST], axis=1)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_test.to_csv(\"./DATA/holdout_set/holdout_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "import sdv\n",
    "from sdv.metadata import Metadata\n",
    "\n",
    "def get_bmi_i(df):\n",
    "    # Calculate BMI for both training and test sets\n",
    "    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "    return df\n",
    "\n",
    "def get_TCTG_i(df):\n",
    "    # Calculate TCTG for both training and test sets\n",
    "    df['TCTG'] = df['TC'] / df['TG']\n",
    "    return df\n",
    "def Synthetic_Data_Generator(df_train, synthesizer = \"TVAE\", epochs = 200, batch_size = 128, n_synthetic_data = 1000): \n",
    "    \"\"\"Conditions: \"balanced\" or None\"\"\"\n",
    "    df_train = df_train.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "    metadata = Metadata.detect_from_dataframe(data=df_train)\n",
    "    metadata.validate()\n",
    "    \n",
    "    #* Synthetic Data generation conditions\n",
    "    condition_list = []\n",
    "    #* Synthesizer setup\n",
    "    if synthesizer == \"CTGAN\":\n",
    "        filepath = f\"{synthesizer}_{epochs}.pkl\"\n",
    "        synthesizer = CTGANSynthesizer(\n",
    "                                metadata=metadata, \n",
    "                                enforce_min_max_values=True, \n",
    "                                enforce_rounding=True, \n",
    "                                epochs = epochs,\n",
    "                                verbose=True, \n",
    "                                cuda=True,\n",
    "                                batch_size=300 # need to be divisible by 10 or pac size\n",
    "                                )  \n",
    "        # df_train = make_divisible(df_train, 10)\n",
    "    elif synthesizer == \"TVAE\":\n",
    "        filepath = f\"{synthesizer}_{epochs}.pkl\"\n",
    "        synthesizer = TVAESynthesizer(\n",
    "                                metadata=metadata, \n",
    "                                enforce_min_max_values=True, \n",
    "                                enforce_rounding=True, \n",
    "                                epochs = epochs,\n",
    "                                verbose=True, \n",
    "                                cuda=True,\n",
    "                                batch_size=batch_size,\n",
    "                                )\n",
    "    else:\n",
    "        return df_train\n",
    "    print(\"Balancing condition applied\")\n",
    "    \n",
    "    # Step 1: Fit the synthesizer\n",
    "    synthesizer.fit(df_train)\n",
    "    \n",
    "    synthesizer.save(filepath)\n",
    "    # Step 2: Get class counts\n",
    "    # Step 1: Get class counts\n",
    "    count_0 = df_train[df_train['DR'] == 0].shape[0]\n",
    "    count_1 = df_train[df_train['DR'] == 1].shape[0]\n",
    "\n",
    "    # Step 2: Balance to the max count\n",
    "    balanced_per_class = max(count_0, count_1)\n",
    "\n",
    "    cond_0 = Condition(column_values={'DR': 0}, num_rows=balanced_per_class - count_0)\n",
    "    cond_1 = Condition(column_values={'DR': 1}, num_rows=balanced_per_class - count_1)\n",
    "\n",
    "    balanced_data = synthesizer.sample_from_conditions([cond_0, cond_1])\n",
    "\n",
    "    # Step 3: Add more *evenly* on top to hit n_synthetic_data\n",
    "    # Note: You already have (balanced_per_class * 2) at this point\n",
    "\n",
    "    current_total = balanced_per_class * 2\n",
    "    remaining = n_synthetic_data - current_total\n",
    "\n",
    "    # Split remaining evenly across classes\n",
    "    extra_per_class = remaining // 2\n",
    "\n",
    "    # (optional: +1 to one class if remaining is odd)\n",
    "    cond_extra_0 = Condition(column_values={'DR': 0}, num_rows=extra_per_class)\n",
    "    cond_extra_1 = Condition(column_values={'DR': 1}, num_rows=remaining - extra_per_class)\n",
    "\n",
    "    extra_data = synthesizer.sample_from_conditions([cond_extra_0, cond_extra_1])\n",
    "\n",
    "    # Step 5: Combine all the synthetic garbage\n",
    "    synthetic_data = pd.concat([balanced_data, extra_data], ignore_index=True)\n",
    "    quality_report = evaluate_quality(df_train, synthetic_data, metadata)\n",
    "    synthetic_data = get_bmi_i(synthetic_data)\n",
    "    synthetic_data = get_TCTG_i(synthetic_data)\n",
    "    \n",
    "    # Ensure folder exists\n",
    "\n",
    "    # Save to specific file\n",
    "    \n",
    "    synthetic_data.to_csv(f\"./DATA/synthetic_training_set/synthetic_data_{epochs}_TVAE.csv\", index=False)\n",
    "\n",
    "    # synthetic_data.to_csv('./synthetic_dataset/synthetic_data2.csv', index=False)\n",
    "    df_train = pd.concat([synthetic_data, df_train], ignore_index=True)\n",
    "    return df_train\n",
    "\n",
    "def FOLDS_GENERATOR_Synthetic(dataset, n_splits=5, random_state=42, \n",
    "                    OD_majority=None, OD_minority=None,\n",
    "                    synthesizer = \"TVAE\", epochs = 200, n_synthetic_data=0, \n",
    "                    scaler=None):\n",
    "    \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    kFolds_list = []\n",
    "\n",
    "    # Convert column names to strings to ensure compatibility\n",
    "    df = dataset.copy()\n",
    "    X = df.drop(columns=[\"DR\"])\n",
    "    Y = pd.DataFrame(df[\"DR\"])\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        train = pd.concat([X.iloc[train_idx], Y.iloc[train_idx]], axis=1)\n",
    "        test = pd.concat([X.iloc[test_idx], Y.iloc[test_idx]], axis=1)\n",
    "        \n",
    "        #* OUTLIER DETECTION\n",
    "        X_train_processed = Outlier_Removal(train, \n",
    "                                            OD_majority=OD_majority,\n",
    "                                            OD_minority=OD_minority,\n",
    "                                            )\n",
    "        \n",
    "        #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "        print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        X_train_processed = Synthetic_Data_Generator(X_train_processed, synthesizer=synthesizer, epochs=epochs, batch_size=512, n_synthetic_data=n_synthetic_data)\n",
    "            \n",
    "        print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        \n",
    "        #* Calculate BMI & ENCODING\n",
    "        # X_train_processed, test = get_bmi(X_train_processed, test)\n",
    "        # X_train_processed, test = get_TCTG(X_train_processed, test)\n",
    "        X_train_processed, test = apply_one_hot_encoding(X_train_processed, test)\n",
    "        #* Scaler\n",
    "        X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "        test[cont_cols] = scaler.transform(test[cont_cols])\n",
    "        # Append processed data (excluding the target column 'DR')\n",
    "        \n",
    "        X_train= X_train_processed.drop(columns=[\"DR\"])\n",
    "        X_tes\n",
    "        \n",
    "        kFolds_list.append((\n",
    "                            X_train_processed.drop(columns=['DR']),\n",
    "                            test.drop(columns=['DR']),\n",
    "                            X_train_processed['DR'].values.reshape(-1, 1),  # Ensures the target is 2D\n",
    "                            test['DR'].values.reshape(-1, 1)  # Ensures the target is 2D\n",
    "                        ))\n",
    "        \n",
    "        break\n",
    "    print(f\"Fold: {fold+1}, Train: {X_train_processed.drop(columns=['DR']).shape}, Test: {test.drop(columns=['DR']).shape}\")\n",
    "    return kFolds_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9429ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: DR\n",
      "0.0    4129\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "After OD, majority: 1911\n",
      "After OD, minority: 210\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    1911\n",
      "1.0     210\n",
      "Name: count, dtype: int64\n",
      "Balancing condition applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAN LE ZHAN\\Documents\\GitHub\\ADL2\\.venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: 2.715: 100%|██████████| 1000/1000 [01:49<00:00,  9.13it/s]\n",
      "Sampling conditions: 100%|██████████| 1701/1701 [00:00<00:00, 3796.88it/s]\n",
      "Sampling conditions: 100%|██████████| 6178/6178 [00:00<00:00, 8761.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 18/18 [00:00<00:00, 59.56it/s]|\n",
      "Column Shapes Score: 84.38%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 153/153 [00:00<00:00, 347.19it/s]|\n",
      "Column Pair Trends Score: 86.75%\n",
      "\n",
      "Overall Score (Average): 85.56%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (10000, 28), Test: (1149, 28)\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "kFolds = FOLDS_GENERATOR_Synthetic(df, n_splits=5, random_state=42,             \n",
    "                            # OD_majority = IsolationForest(contamination=trial.suggest_float(\"contamination_majority\", 0.01, 0.4), random_state=random_state, \n",
    "                            OD_majority = IQRDetector(factor=1),\n",
    "                            OD_minority = IQRDetector(factor=1),\n",
    "         \n",
    "                            synthesizer = \"TVAE\",\n",
    "                            epochs = 1000,\n",
    "                            n_synthetic_data = 10000,\n",
    "                            scaler=scaler,      \n",
    "                            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc820c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33m./DATA/synthetic_training_set/synthetic_data_2000_TVAE.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m df.describe()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./DATA/synthetic_training_set/synthetic_data_2000_TVAE.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cea845",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msdv\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msdv\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Metadata\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m synthetic_data = \u001b[43mread_csv\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m./synthetic_dataset/synthetic_data_only.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m metadata = Metadata.detect_from_dataframe(data=real_data)\n\u001b[32m      8\u001b[39m metadata.validate()\n",
      "\u001b[31mNameError\u001b[39m: name 'read_csv' is not defined"
     ]
    }
   ],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "import sdv\n",
    "from sdv.metadata import Metadata\n",
    "\n",
    "synthetic_data = read_csv(\"./synthetic_dataset/synthetic_data_only.csv\")\n",
    "metadata = Metadata.detect_from_dataframe(data=real_data)\n",
    "metadata.validate()\n",
    "metadata.visualize()\n",
    "# 1. perform basic validity checks\n",
    "diagnostic = run_diagnostic(df, synthetic_data, metadata)\n",
    "\n",
    "# 2. measure the statistical similarity\n",
    "quality_report = evaluate_quality(real_data, synthetic_data, metadata)\n",
    "\n",
    "# # 3. plot the data\n",
    "# fig = get_column_plot(\n",
    "#     real_data=real_data,\n",
    "#     synthetic_data=synthetic_data,\n",
    "#     metadata=metadata,\n",
    "#     column_name='Age' #change this u decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e767b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
