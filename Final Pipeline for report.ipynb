{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f6aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #?  change which dataset to take from to train\n",
    "# import pandas as pd\n",
    "# training_set = pd.read_csv('./DATA_OVERFIT/folds/train_fold_0.csv')\n",
    "# cv_set = pd.read_csv('./DATA_OVERFIT/folds/test_fold_0.csv')\n",
    "# holdout_set = pd.read_csv('./DATA_0/holdout_set/holdout_data_OHE.csv')\n",
    "# print(training_set.shape)   \n",
    "# print(cv_set.shape)\n",
    "# print(holdout_set.shape)\n",
    "\n",
    "# train_x = training_set.drop(columns = 'DR')\n",
    "# train_y = training_set[['DR']]\n",
    "# test_x = cv_set.drop(columns = 'DR')\n",
    "# test_y = cv_set[['DR']]       \n",
    "        \n",
    "# kFolds = train_x, test_x, train_y, test_y\n",
    "# for i in kFolds:\n",
    "#     print(i.shape)\n",
    "    \n",
    "    \n",
    "# #! need to change the directories below because the actual call is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7408c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device, threshold = 0.5):\n",
    "    # if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "    #     raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "    n_count = 0\n",
    "    criterion.to(device) #? Move criterion to device\n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            n_count += inputs.size(0) #? Count number of samples trained\n",
    "                                    \n",
    "            # print(all(param.requires_grad for param in model.parameters()))\n",
    "   \n",
    "            torch.set_printoptions(threshold=float('inf'))\n",
    "            \n",
    "            assert not torch.isnan(inputs).any(), \"Input has NaNs\"\n",
    "            assert not torch.isinf(inputs).any(), \"Input has Infs\"\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            assert not torch.isnan(outputs).any(), \"Model output has NaNs\"\n",
    "            assert not torch.isinf(outputs).any(), \"Model output has Infs\"\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            assert not torch.isnan(loss).any(), \"Model loss has NaNs\"\n",
    "            loss.backward() #? Backpropagation\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            # scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                # labels = labels.cpu() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            loss_ratio = val_loss / train_loss    \n",
    "            pos_weight = loss_ratio  # or any other function of loss_ratio you choose\n",
    "            # scheduler.step(val_loss)  # Use validation loss here, not training loss\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update criterion with new pos_weight\n",
    "        # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss and epoch > 100:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        elif avg_val_loss*0.96 <= best_val_loss:\n",
    "                wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}\".ljust(12), \n",
    "              f\"training loss:{train_loss:.3f}\".ljust(16), \n",
    "              f\"best_val_loss:{best_val_loss:.3f}\".ljust(12), \n",
    "              f\"Val Loss: {avg_val_loss:.3f}\",\n",
    "              f\"Scheduler lr: {scheduler.get_last_lr()}\".ljust(50), \n",
    "              f\"N samples trained: {n_count}\",\n",
    "              end=\"\\r\")\n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                labels = labels.cpu() \n",
    "                # predictions = (torch.sigmoid(outputs) < 0.5).float().cpu().numpy()\n",
    "                predictions = (torch.sigmoid(outputs) >= threshold).float().cpu().numpy()\n",
    "                \n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy=accuracy_score(labels, predictions) \n",
    "    precision=precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
    "    recall=recall_score(labels, predictions, pos_label=1)\n",
    "    f1=f1_score(labels, predictions, pos_label=1)\n",
    "    auc=roc_auc_score(labels, predictions)\n",
    "    \n",
    "    return model, accuracy, precision, recall, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7733ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class Ivan_NN(nn.Module):\n",
    "#     def __init__(self, input_dim,):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(input_dim,256),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.Linear(256,2048),\n",
    "#             nn.BatchNorm1d(2048),\n",
    "#             nn.LeakyReLU(),\n",
    "#             # nn.Dropout(0.2),\n",
    "#             nn.Linear(2048,712),\n",
    "#             nn.BatchNorm1d(712),\n",
    "#             nn.LeakyReLU(),\n",
    "#             # # nn.Dropout(0.2),\n",
    "#             nn.Linear(712,360),\n",
    "#             nn.BatchNorm1d(360),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(360,512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(512,1024),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(1024,512),\n",
    "#             nn.LeakyReLU(),\n",
    "#             # nn.LeakyReLU(),\n",
    "#             # nn.Dropout(0.2),\n",
    "#             nn.Linear(512,324),\n",
    "#             nn.BatchNorm1d(324),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(324,64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64,1),\n",
    "#             # nn.LeakyReLU(0.2),\n",
    "#             # nn.Linear(32,1),\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         return self.layers(x)\n",
    "\n",
    "# print(Ivan_NN(28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591f9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Ivan_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, 2180)\n",
    "        self.input_bn = nn.BatchNorm1d(2180)\n",
    "    \n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(2180, 888),\n",
    "            nn.BatchNorm1d(888),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(888, 1122),\n",
    "            nn.BatchNorm1d(1122),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(1122, 624),\n",
    "            nn.BatchNorm1d(624),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(624, 1080),\n",
    "            nn.BatchNorm1d(1080),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(1080, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),  # â† Fixed this\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Linear(128, 36),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Linear(36, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),  # or LeakyReLU, up to you\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Skip connection projectors\n",
    "        self.skip1_proj = nn.Sequential(nn.Linear(2180, 1122))\n",
    "        self.skip2_proj = nn.Sequential(nn.Linear(1122, 128))\n",
    "        self.skip3_proj = nn.Sequential(nn.Linear(128, 64))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.input_bn(x)\n",
    "        # x = torch.nn.functional.leaky_relu(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "\n",
    "        skip1 = self.skip1_proj(x)\n",
    "        x2 = x2 + skip1\n",
    "\n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        x5 = self.block5(x4)\n",
    "\n",
    "        x6 = self.block6(x5)\n",
    "\n",
    "        skip2 = self.skip2_proj(x2)\n",
    "        x6 = x6 + skip2\n",
    "\n",
    "        x7 = self.block7(x6)\n",
    "        x8 = self.block8(x7)\n",
    "\n",
    "        skip3 = self.skip3_proj(x6)\n",
    "        x8 = x8 + skip3\n",
    "\n",
    "        out = self.output(x8)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b59f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivan_NN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Linear(in_features=64, out_features=80, bias=True)\n",
      "    (10): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2)\n",
      "    (12): Linear(in_features=80, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Ivan_NN(nn.Module):\n",
    "    def __init__(self, input_dim,):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim,64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            # # nn.Dropout(0.2),\n",
    "            nn.Linear(64,80),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # nn.Linear(360,512),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Linear(512,1024),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Linear(1024,512),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            # nn.Linear(512,324),\n",
    "            # nn.BatchNorm1d(324),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Linear(324,64),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.Linear(80,1),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Linear(32,1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        return self.layers(x)\n",
    "\n",
    "print(Ivan_NN(28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c560cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training_Helper_Functions import *\n",
    "\n",
    "\n",
    "#! change imports\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch import optim\n",
    "def maximise_combined_score(trial):\n",
    "    if True:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", device)\n",
    "        epochs = 800\n",
    "        random_state = 42\n",
    "        # Scaler\n",
    "        scaler = RobustScaler() # Or StandardScaler() - You can also make this a hyperparameter if you want\n",
    "        threshold = trial.suggest_float(\"threshold\", 0.28,0.6,)\n",
    "        # dropout = None\n",
    "        initial_lr = trial.suggest_float(\"initial_lr\", 1e-3, 1e-3 ,log=True)\n",
    "        max_lr = trial.suggest_float(\"max_lr\", 1e-3, 1e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 6e-1, log=True)\n",
    "        # Loss function hyperparameters\n",
    "        criterion_choice = \"BCEWithLogitsLoss\" #trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\", \"FocalLoss\"])\n",
    "        # Hyperparameter exploration optimization\n",
    "        if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "            pos_weight = trial.suggest_int(\"pos_weight\",1, 1)\n",
    "            alpha = None\n",
    "            gamma = None\n",
    "        elif criterion_choice == \"FocalLoss\":\n",
    "            pos_weight = None\n",
    "            alpha = trial.suggest_float(\"alpha\", 0.25, 0.75)\n",
    "            gamma = trial.suggest_float(\"gamma\", 1.0, 5.0)\n",
    "        else:\n",
    "            pos_weight = None\n",
    "\n",
    "        # Initialize lists for metrics across folds\n",
    "        accuracy_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        auc_list = []\n",
    "    if True:\n",
    "        training_set = pd.read_csv('./DATA/folds/train_fold_0.csv')\n",
    "        cv_set = pd.read_csv('./DATA/folds/test_fold_0.csv')\n",
    "        holdout_set = pd.read_csv('./DATA/holdout_set/holdout_data_OHE.csv')\n",
    "        # print(training_set.shape)\n",
    "        # print(cv_set.shape)\n",
    "        # print(holdout_set.shape)\n",
    "\n",
    "        train_x = training_set.drop(columns = 'DR')\n",
    "        train_y = training_set[['DR']]\n",
    "        test_x = cv_set.drop(columns = 'DR')\n",
    "        test_y = cv_set[['DR']] \n",
    "    # Cross-validation loop\n",
    "    # for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "    train_x, test_x, train_y, test_y\n",
    "    fold = 1\n",
    "    print(f\"Fold {fold}:\")\n",
    "    # Create DataLoader for current fold\n",
    "    train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=512,\n",
    "                                                        device=device)\n",
    "    # Instantiate and initialize the model\n",
    "    model = Ivan_NN(input_dim=get_feature_count(train_loader))\n",
    "    model.to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # Map the choice to the actual loss function\n",
    "    criterion = criterion_mapping(criterion_choice, pos_weight, alpha, gamma)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay) \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "        optimiser,\n",
    "        base_lr=1e-6,\n",
    "        max_lr=max_lr,\n",
    "        cycle_momentum=True)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.1)\n",
    "\n",
    "    # Train and evaluate the model on the current fold\n",
    "    model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "        model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=epochs, patience=40,\n",
    "        device=device, threshold=threshold\n",
    "    )\n",
    "    print(f\"Accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, auc: {auc:.4f}\")\n",
    "    del model\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "\n",
    "    # Append the metrics from the current fold\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    # combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "    combined_score = avg_f1\n",
    "\n",
    "    return combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25ba0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:50:15,772] A new study created in memory with name: Basic\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 145, best val loss: 1.6228s: 2.482 Scheduler lr: [7.292800000000007e-05]              N samples trained: 2580480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:50:59,167] Trial 0 finished with value: 0.31210191082802546 and parameters: {'threshold': 0.5907571683338007, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 2.0806914493899644e-05, 'pos_weight': 1}. Best is trial 0 with value: 0.31210191082802546.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8120, precision: 0.2475, recall: 0.4224, f1: 0.3121, auc: 0.6391\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 140   training loss:0.096 best_val_loss:1.453 Val Loss: 2.062 Scheduler lr: [7.092999999999985e-05]              N samples trained: 2508800\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:51:44,578] Trial 1 finished with value: 0.3227848101265823 and parameters: {'threshold': 0.597704452522527, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.006300158315291185, 'pos_weight': 1}. Best is trial 1 with value: 0.3227848101265823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 141, best val loss: 1.4531\n",
      "Accuracy: 0.8138, precision: 0.2550, recall: 0.4397, f1: 0.3228, auc: 0.6477\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 143   training loss:0.089 best_val_loss:1.945 Val Loss: 2.759 Scheduler lr: [7.24284999999999e-05]               N samples trained: 2562560\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:52:23,016] Trial 2 finished with value: 0.2890855457227139 and parameters: {'threshold': 0.5786562412662332, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.0002503660696454788, 'pos_weight': 1}. Best is trial 1 with value: 0.3227848101265823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 144, best val loss: 1.9448\n",
      "Accuracy: 0.7903, precision: 0.2197, recall: 0.4224, f1: 0.2891, auc: 0.6270\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 181   training loss:0.037 best_val_loss:1.246 Val Loss: 1.875 Scheduler lr: [9.140950000000003e-05]              N samples trained: 3243520\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:53:22,073] Trial 3 finished with value: 0.2988505747126437 and parameters: {'threshold': 0.40420744599967495, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.0036351239449815055, 'pos_weight': 1}. Best is trial 1 with value: 0.3227848101265823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 182, best val loss: 1.2462\n",
      "Accuracy: 0.7876, precision: 0.2241, recall: 0.4483, f1: 0.2989, auc: 0.6370\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 286   training loss:0.389 best_val_loss:0.465 Val Loss: 0.552 Scheduler lr: [0.0001438569999999998]              N samples trained: 5125120\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:54:44,055] Trial 4 finished with value: 0.31932773109243695 and parameters: {'threshold': 0.46079583482241127, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.28768468779280204, 'pos_weight': 1}. Best is trial 1 with value: 0.3227848101265823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 287, best val loss: 0.4652\n",
      "Accuracy: 0.7885, precision: 0.2365, recall: 0.4914, f1: 0.3193, auc: 0.6566\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 201   training loss:0.054 best_val_loss:1.163 Val Loss: 1.235 Scheduler lr: [0.00010139949999999981]             N samples trained: 3601920\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:55:42,367] Trial 5 finished with value: 0.32558139534883723 and parameters: {'threshold': 0.4684235082730942, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.03597298737009726, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 202, best val loss: 1.1628\n",
      "Accuracy: 0.7981, precision: 0.2456, recall: 0.4828, f1: 0.3256, auc: 0.6581\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 148, best val loss: 1.4946s: 2.019 Scheduler lr: [7.442650000000013e-05]              N samples trained: 2634240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:56:24,386] Trial 6 finished with value: 0.30952380952380953 and parameters: {'threshold': 0.48420096337145707, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 1.6766184049406895e-05, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7981, precision: 0.2364, recall: 0.4483, f1: 0.3095, auc: 0.6428\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 146   training loss:0.114 best_val_loss:0.778 Val Loss: 0.861 Scheduler lr: [7.392699999999996e-05]              N samples trained: 2616320\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:57:02,014] Trial 7 finished with value: 0.2857142857142857 and parameters: {'threshold': 0.5531150083527293, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.02935004023693843, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 147, best val loss: 0.7781\n",
      "Accuracy: 0.7955, precision: 0.2207, recall: 0.4052, f1: 0.2857, auc: 0.6222\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 151   training loss:0.092 best_val_loss:1.538 Val Loss: 1.857 Scheduler lr: [7.642449999999991e-05]              N samples trained: 2705920\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:57:46,170] Trial 8 finished with value: 0.30517711171662126 and parameters: {'threshold': 0.39400143178113434, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.010095326107519864, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 152, best val loss: 1.5380\n",
      "Accuracy: 0.7781, precision: 0.2231, recall: 0.4828, f1: 0.3052, auc: 0.6470\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 147   training loss:0.082 best_val_loss:1.082 Val Loss: 1.365 Scheduler lr: [7.442650000000013e-05]              N samples trained: 2634240\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:58:21,723] Trial 9 finished with value: 0.30177514792899407 and parameters: {'threshold': 0.455792286879118, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.00587242911192485, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 148, best val loss: 1.0820\n",
      "Accuracy: 0.7946, precision: 0.2297, recall: 0.4397, f1: 0.3018, auc: 0.6371\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 188   training loss:0.257 best_val_loss:0.573 Val Loss: 0.657 Scheduler lr: [9.490599999999987e-05]              N samples trained: 3368960\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 04:59:20,236] Trial 10 finished with value: 0.30707070707070705 and parameters: {'threshold': 0.32278347640819455, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.4940717069133068, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 189, best val loss: 0.5731\n",
      "Accuracy: 0.7015, precision: 0.2005, recall: 0.6552, f1: 0.3071, auc: 0.6809\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 310, best val loss: 0.4650s: 0.790 Scheduler lr: [0.0001553455000000001]              N samples trained: 5537280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 05:00:45,664] Trial 11 finished with value: 0.2857142857142857 and parameters: {'threshold': 0.5275996144370525, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.06611190513155595, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8042, precision: 0.2261, recall: 0.3879, f1: 0.2857, auc: 0.6194\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 178   training loss:0.048 best_val_loss:1.843 Val Loss: 2.154 Scheduler lr: [8.991099999999997e-05]              N samples trained: 3189760\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 05:01:33,829] Trial 12 finished with value: 0.31232876712328766 and parameters: {'threshold': 0.3015339927087317, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.0005392550763507689, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 179, best val loss: 1.8427\n",
      "Accuracy: 0.7815, precision: 0.2289, recall: 0.4914, f1: 0.3123, auc: 0.6528\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 144   training loss:0.092 best_val_loss:1.686 Val Loss: 2.317 Scheduler lr: [7.292800000000007e-05]              N samples trained: 2580480\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 05:06:42,964] Trial 13 finished with value: 0.32294617563739375 and parameters: {'threshold': 0.5156484828990211, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.0008411670512029099, 'pos_weight': 1}. Best is trial 5 with value: 0.32558139534883723.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 145, best val loss: 1.6856\n",
      "Accuracy: 0.7920, precision: 0.2405, recall: 0.4914, f1: 0.3229, auc: 0.6586\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 148   training loss:0.085 best_val_loss:1.672 Val Loss: 2.074 Scheduler lr: [7.492599999999984e-05]              N samples trained: 2652160\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 05:07:11,327] Trial 14 finished with value: 0.3352941176470588 and parameters: {'threshold': 0.5083474884116073, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.0004696687678473499, 'pos_weight': 1}. Best is trial 14 with value: 0.3352941176470588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 149, best val loss: 1.6724\n",
      "Accuracy: 0.8033, precision: 0.2545, recall: 0.4914, f1: 0.3353, auc: 0.6649\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 117   training loss:0.121 best_val_loss:1.738 Val Loss: 1.926 Scheduler lr: [5.94415e-05]                        N samples trained: 2096640\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-23 05:07:34,980] Trial 15 failed with parameters: {'threshold': 0.4143713380165521, 'initial_lr': 0.001, 'max_lr': 0.001, 'weight_decay': 0.00011541745858379638, 'pos_weight': 1} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_5988\\3184159513.py\", line 81, in maximise_combined_score\n",
      "    model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_5988\\3861305712.py\", line 30, in train_and_evaluate\n",
      "    outputs = model(inputs) #? Forward pass through the model\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1735, in _wrapped_call_impl\n",
      "    def _wrapped_call_impl(self, *args, **kwargs):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "[W 2025-04-23 05:07:34,981] Trial 15 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118   training loss:0.118 best_val_loss:1.738 Val Loss: 1.964 Scheduler lr: [5.9941000000000164e-05]             N samples trained: 2114560\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m dashboard_thread.start()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaximise_combined_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# After optimization, print results\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mmaximise_combined_score\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     71\u001b[39m scheduler = torch.optim.lr_scheduler.CyclicLR(\n\u001b[32m     72\u001b[39m     optimiser,\n\u001b[32m     73\u001b[39m     base_lr=\u001b[32m1e-6\u001b[39m,\n\u001b[32m     74\u001b[39m     max_lr=max_lr,\n\u001b[32m     75\u001b[39m     cycle_momentum=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=3, factor=0.5)\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.1)\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Train and evaluate the model on the current fold\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m model, accuracy, precision, recall, f1, auc = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, f1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, auc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs, patience, device, threshold)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.isnan(inputs).any(), \u001b[33m\"\u001b[39m\u001b[33mInput has NaNs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.isinf(inputs).any(), \u001b[33m\"\u001b[39m\u001b[33mInput has Infs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#? Forward pass through the model\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.isnan(outputs).any(), \u001b[33m\"\u001b[39m\u001b[33mModel output has NaNs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.isinf(outputs).any(), \u001b[33m\"\u001b[39m\u001b[33mModel output has Infs\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1735\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1732\u001b[39m             tracing_state.pop_scope()\n\u001b[32m   1733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m1735\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1736\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "def start_dashboard():\n",
    "    run_server(storage)\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "study = optuna.create_study(direction=\"maximize\", storage=storage, study_name=\"Basic\")\n",
    "\n",
    "# Start dashboard in a separate thread\n",
    "dashboard_thread = threading.Thread(target=start_dashboard, daemon=True)\n",
    "dashboard_thread.start()\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(maximise_combined_score, n_trials=30)\n",
    "\n",
    "# After optimization, print results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8e273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc34e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
