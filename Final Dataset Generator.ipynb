{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028354d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "from Training_Helper_Functions import *\n",
    "from Preprocessing_Functions2 import * \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf1922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "import os\n",
    "os.makedirs(\"./DATA/folds\", exist_ok=True)\n",
    "os.makedirs(\"./DATA/holdout_set\", exist_ok=True)\n",
    "os.makedirs(\"./DATA/synthetic_training_set\", exist_ok=True)\n",
    "os.makedirs(\"./DATA/training_set\", exist_ok=True)\n",
    "# Load and split dataset\n",
    "data = pd.read_csv(\"./original_dataset/processed_data_encoded.csv\")\n",
    "X = data.drop(columns=[\"DR\"])  # Keep BMI and TCTG if you're removing them later\n",
    "Y = data[\"DR\"]\n",
    "\n",
    "X_folds, X_test, Y_folds, Y_test = train_test_split(X, Y, test_size=0.1, stratify=Y, random_state=random_state)\n",
    "\n",
    "# Save training and holdout sets\n",
    "pd.concat([X_folds, Y_folds], axis=1).reset_index(drop=True).to_csv(\"./DATA/training_set/training_data.csv\", index=False)\n",
    "pd.concat([X_test, Y_test], axis=1).reset_index(drop=True).to_csv(\"./DATA/holdout_set/holdout_data.csv\", index=False)\n",
    "\n",
    "# Apply encoding\n",
    "df_train, df_test = apply_one_hot_encoding(\n",
    "    pd.concat([X_folds, Y_folds], axis=1).reset_index(drop=True),\n",
    "    pd.concat([X_test, Y_test], axis=1).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_test.to_csv(\"./DATA/holdout_set/holdout_data_OHE.csv\", index=False) #only do OHE for holdout set for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58ec440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Outlier_Removal(df_train, OD_majority, OD_minority): \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "\n",
    "    print(\"Original class distribution:\",df_train[y_col].value_counts())\n",
    "    assert y_col in df_train.columns, f\"'{y_col}' column is missing in the DataFrame.\"\n",
    "    \n",
    "    #* OUTLIER DETECTION START\n",
    "    available_cont_cols = [col for col in cont_cols if col in df_train.columns]\n",
    "    df_majority = df_train[df_train[y_col] == 0].copy()\n",
    "    df_minority = df_train[df_train[y_col] == 1].copy()\n",
    "    if OD_majority is not None:\n",
    "        outliers_majority = OD_majority.fit_predict(df_majority[available_cont_cols])\n",
    "        df_majority = df_majority[outliers_majority == 1]\n",
    "        print(f\"After OD, majority: {len(df_majority)}\")\n",
    "    if OD_minority is not None:\n",
    "        outliers_minority = OD_minority.fit_predict(df_minority[available_cont_cols])\n",
    "        df_minority = df_minority[outliers_minority == 1]\n",
    "        print(f\"After OD, minority: {len(df_minority)}\")\n",
    "    df_after_OD = pd.concat([df_majority, df_minority], ignore_index=True)\n",
    "    #* OUTLIER DETECTION END - df_after_OD is the new df\n",
    "    return df_after_OD\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "def apply_smotenc_oversampling(df_train):\n",
    "    \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "\n",
    "    print(\"\\nApplying SMOTENC oversampling...\")\n",
    "\n",
    "    # Split features and label\n",
    "    X = df_train.drop(columns=[y_col])\n",
    "    y = df_train[y_col]\n",
    "\n",
    "    # Find indices of categorical features\n",
    "    cat_indices = [X.columns.get_loc(col) for col in cat_cols if col in X.columns]\n",
    "\n",
    "    # Ensure 'Community' is integer type if present\n",
    "    if 'Community' in X.columns:\n",
    "        X['Community'] = X['Community'].astype(int)\n",
    "\n",
    "    oversampler = SMOTENC(categorical_features=cat_indices, random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    print(pd.DataFrame(X_resampled, columns=X.columns).describe())\n",
    "    print(\"\\nFinal class distribution:\", pd.Series(y_resampled).value_counts())\n",
    "\n",
    "    # Recombine into a single DataFrame\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[y_col] = y_resampled\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def FOLDS_GENERATOR1(df:pd.DataFrame, n_splits=5, random_state=42, OD_majority=None, OD_minority=None,\n",
    "                    synthesizer = \"TVAE\", epochs = 100, batch_size=512, n_synthetic_data=0,\n",
    "                    scaler=None):\n",
    "    \"\"\"\n",
    "    Generates n_splits folds for cross-validation.\n",
    "    \"\"\"\n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    # Create a StratifiedKFold object\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    # Initialize a list to hold the folds\n",
    "    df_copy = df.copy()\n",
    "    X = df_copy.drop(columns=[\"DR\"])\n",
    "    Y = df[[\"DR\"]]\n",
    "    X = X.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "\n",
    "    kFolds_list = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        train = pd.concat([X.iloc[train_idx], Y.iloc[train_idx]], axis=1)\n",
    "        test = pd.concat([X.iloc[test_idx], Y.iloc[test_idx]], axis=1)\n",
    "        \n",
    "        # #* OUTLIER DETECTION\n",
    "        X_train_processed = Outlier_Removal(train, \n",
    "                                            OD_majority=OD_majority,\n",
    "                                            OD_minority=OD_minority,\n",
    "                                            )\n",
    "        X_train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "        # #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "        print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        X_train_processed = Synthetic_Data_Generator2(X_train_processed, fold, synthesizer=synthesizer, epochs=epochs, batch_size=512, n_synthetic_data=n_synthetic_data)\n",
    "        \n",
    "        \n",
    "        print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        X_train_processed = train.copy()\n",
    "        \n",
    "        #* Calculate BMI, TCTG & ENCODING\n",
    "        X_train_processed, test = get_bmi(X_train_processed, test)\n",
    "        X_train_processed, test = get_TCTG(X_train_processed, test)\n",
    "        X_train_processed, test = apply_one_hot_encoding(X_train_processed, test)\n",
    "        #* Scaler\n",
    "        X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "        test[cont_cols] = scaler.transform(test[cont_cols])\n",
    "        # Append processed data (excluding the target column 'DR')\n",
    "\n",
    "        # Save to CSV with fold number\n",
    "        X_train_processed.to_csv(f\"./DATA/folds/train_fold_{fold}.csv\", index=False)\n",
    "        test.to_csv(f\"./DATA/folds/test_fold_{fold}.csv\", index=False)\n",
    "        kFolds_list.append((\n",
    "                            X_train_processed.drop(columns=['DR']),\n",
    "                            test.drop(columns=['DR']),\n",
    "                            X_train_processed['DR'].values.reshape(-1, 1),  # Ensures the target is 2D\n",
    "                            test['DR'].values.reshape(-1, 1)  # Ensures the target is 2D\n",
    "                        ))\n",
    "        break\n",
    "    print(f\"Fold: {fold+1}, Train: {X_train_processed.drop(columns=['DR']).shape}, Test: {test.drop(columns=['DR']).shape}\")\n",
    "    \n",
    "    return kFolds_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68320b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: DR\n",
      "0.0    4129\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "After OD, majority: 2136\n",
      "After OD, minority: 377\n",
      "\n",
      "Applying SMOTENC oversampling...\n",
      "               Age       Gender    Community         UAlb           Ucr  \\\n",
      "count  4272.000000  4272.000000  4272.000000  4272.000000   4272.000000   \n",
      "mean     63.407783     0.538858     4.034176    17.776940   3601.733876   \n",
      "std       6.674629     0.498546     3.094886    24.725244   5182.172389   \n",
      "min      36.000000     0.000000     0.000000     0.100000      1.000000   \n",
      "25%      59.000000     0.000000     1.000000     4.600000      6.000000   \n",
      "50%      63.752691     1.000000     4.000000     9.250930     11.000000   \n",
      "75%      68.000000     1.000000     7.000000    19.664959   6899.750000   \n",
      "max      87.000000     1.000000     9.000000   174.800000  19307.000000   \n",
      "\n",
      "              UACR           TC           TG         LDLC         HDLC  \\\n",
      "count  4272.000000  4272.000000  4272.000000  4272.000000  4272.000000   \n",
      "mean     19.693311     5.219161     1.419700     3.233704     1.353745   \n",
      "std      26.690342     0.877394     0.682973     0.789194     0.259540   \n",
      "min       0.100000     2.640000     0.250000     1.190000     0.720000   \n",
      "25%       5.500000     4.606855     0.920000     2.686619     1.165880   \n",
      "50%      10.400000     5.192315     1.270000     3.230000     1.323151   \n",
      "75%      23.000000     5.791692     1.790000     3.740000     1.520000   \n",
      "max     211.100000     9.280000     4.260000     6.520000     2.610000   \n",
      "\n",
      "               Scr          BUN          FPG        HbA1c       Height  \\\n",
      "count  4272.000000  4272.000000  4272.000000  4272.000000  4272.000000   \n",
      "mean     58.643538     5.858224     9.146199     7.397950   161.516277   \n",
      "std      12.659837     1.316172     2.448516     1.303607     7.022758   \n",
      "min      28.000000     2.600000     4.400000     4.400000   139.000000   \n",
      "25%      49.124377     4.928468     7.400000     6.400000   156.248493   \n",
      "50%      57.500324     5.700000     8.700000     7.200000   161.000000   \n",
      "75%      66.327592     6.610492    10.400000     8.200000   167.000000   \n",
      "max     108.000000    12.000000    20.500000    13.700000   180.000000   \n",
      "\n",
      "            Weight     Duration  \n",
      "count  4272.000000  4272.000000  \n",
      "mean     63.278341     8.498744  \n",
      "std       8.444757     5.659164  \n",
      "min      37.000000     0.100000  \n",
      "25%      57.000000     3.800000  \n",
      "50%      63.000000     7.515457  \n",
      "75%      69.634127    13.000000  \n",
      "max      98.000000    33.000000  \n",
      "\n",
      "Final class distribution: DR\n",
      "0.0    2136\n",
      "1.0    2136\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    2136\n",
      "1.0    2136\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: 3.985: 100%|██████████| 1000/1000 [01:50<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 5000/5000 [00:00<00:00, 16825.93it/s]\n",
      "Sampling conditions: 100%|██████████| 5000/5000 [00:00<00:00, 16922.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 18/18 [00:00<00:00, 31.27it/s]|\n",
      "Column Shapes Score: 91.57%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 153/153 [00:00<00:00, 429.19it/s]|\n",
      "Column Pair Trends Score: 93.84%\n",
      "\n",
      "Overall Score (Average): 92.7%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    7136\n",
      "1.0    7136\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (4593, 28), Test: (1149, 28)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaler = RobustScaler()\n",
    "scaler = StandardScaler()\n",
    "training_set = pd.read_csv(\"./DATA/training_set/training_data.csv\")\n",
    "\n",
    "kFolds = FOLDS_GENERATOR1(training_set, n_splits=5, random_state=42,             \n",
    "                            OD_majority = IQRDetector(factor=1),\n",
    "                            OD_minority = IQRDetector(factor=2.5),\n",
    "                            synthesizer = \"TVAE\",\n",
    "                            epochs = 1000,\n",
    "                            n_synthetic_data = 10000,\n",
    "                            scaler=scaler,      \n",
    "                            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe1b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa605a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
