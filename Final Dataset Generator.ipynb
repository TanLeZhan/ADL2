{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028354d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training_Helper_Functions import *\n",
    "from Preprocessing_Functions2 import * \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf1922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "import os\n",
    "os.makedirs(\"./DATA/folds\", exist_ok=True)\n",
    "os.makedirs(\"./DATA/holdout_set\", exist_ok=True)\n",
    "os.makedirs(\"./DATA/synthetic_training_set\", exist_ok=True)\n",
    "os.makedirs(\"./DATA/training_set\", exist_ok=True)\n",
    "# Load and split dataset\n",
    "data = pd.read_csv(\"./original_dataset/processed_data_encoded.csv\")\n",
    "X = data.drop(columns=[\"DR\"])  # Keep BMI and TCTG if you're removing them later\n",
    "Y = data[\"DR\"]\n",
    "\n",
    "X_folds, X_test, Y_folds, Y_test = train_test_split(X, Y, test_size=0.1, stratify=Y, random_state=random_state)\n",
    "\n",
    "# Save training and holdout sets\n",
    "pd.concat([X_folds, Y_folds], axis=1).reset_index(drop=True).to_csv(\"./DATA/training_set/training_data.csv\", index=False)\n",
    "pd.concat([X_test, Y_test], axis=1).reset_index(drop=True).to_csv(\"./DATA/holdout_set/holdout_data.csv\", index=False)\n",
    "\n",
    "# Apply encoding\n",
    "df_train, df_test = apply_one_hot_encoding(\n",
    "    pd.concat([X_folds, Y_folds], axis=1).reset_index(drop=True),\n",
    "    pd.concat([X_test, Y_test], axis=1).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_test.to_csv(\"./DATA/holdout_set/holdout_data_OHE.csv\", index=False) #only do OHE for holdout set for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58ec440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Outlier_Removal(df_train, OD_majority, OD_minority): \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "\n",
    "    print(\"Original class distribution:\",df_train[y_col].value_counts())\n",
    "    assert y_col in df_train.columns, f\"'{y_col}' column is missing in the DataFrame.\"\n",
    "    \n",
    "    #* OUTLIER DETECTION START\n",
    "    available_cont_cols = [col for col in cont_cols if col in df_train.columns]\n",
    "    df_majority = df_train[df_train[y_col] == 0].copy()\n",
    "    df_minority = df_train[df_train[y_col] == 1].copy()\n",
    "    if OD_majority is not None:\n",
    "        outliers_majority = OD_majority.fit_predict(df_majority[available_cont_cols])\n",
    "        df_majority = df_majority[outliers_majority == 1]\n",
    "        print(f\"After OD, majority: {len(df_majority)}\")\n",
    "    if OD_minority is not None:\n",
    "        outliers_minority = OD_minority.fit_predict(df_minority[available_cont_cols])\n",
    "        df_minority = df_minority[outliers_minority == 1]\n",
    "        print(f\"After OD, minority: {len(df_minority)}\")\n",
    "    df_after_OD = pd.concat([df_majority, df_minority], ignore_index=True)\n",
    "    #* OUTLIER DETECTION END - df_after_OD is the new df\n",
    "    return df_after_OD\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "def apply_smotenc_oversampling(df_train):\n",
    "    \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    # Use the original encoded single column name here\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    y_col = 'DR'\n",
    "\n",
    "    print(\"\\nApplying SMOTENC oversampling...\")\n",
    "\n",
    "    # Split features and label\n",
    "    X = df_train.drop(columns=[y_col])\n",
    "    y = df_train[y_col]\n",
    "\n",
    "    # Find indices of categorical features\n",
    "    cat_indices = [X.columns.get_loc(col) for col in cat_cols if col in X.columns]\n",
    "\n",
    "    # Ensure 'Community' is integer type if present\n",
    "    if 'Community' in X.columns:\n",
    "        X['Community'] = X['Community'].astype(int)\n",
    "\n",
    "    oversampler = SMOTENC(categorical_features=cat_indices, random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    print(pd.DataFrame(X_resampled, columns=X.columns).describe())\n",
    "    print(\"\\nFinal class distribution:\", pd.Series(y_resampled).value_counts())\n",
    "\n",
    "    # Recombine into a single DataFrame\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[y_col] = y_resampled\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def FOLDS_GENERATOR1(df:pd.DataFrame, n_splits=5, random_state=42, OD_majority=None, OD_minority=None,\n",
    "                    synthesizer = \"TVAE\", epochs = 100, batch_size=512, n_synthetic_data=0,\n",
    "                    scaler=None):\n",
    "    \"\"\"\n",
    "    Generates n_splits folds for cross-validation.\n",
    "    \"\"\"\n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    cat_cols = ['Gender', 'Community'] \n",
    "    # Create a StratifiedKFold object\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    # Initialize a list to hold the folds\n",
    "    df_copy = df.copy()\n",
    "    X = df_copy.drop(columns=[\"DR\"])\n",
    "    Y = df[[\"DR\"]]\n",
    "    X = X.drop(columns=[\"BMI\", \"TCTG\"])\n",
    "\n",
    "    kFolds_list = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        train = pd.concat([X.iloc[train_idx], Y.iloc[train_idx]], axis=1)\n",
    "        test = pd.concat([X.iloc[test_idx], Y.iloc[test_idx]], axis=1)\n",
    "        \n",
    "        # #* OUTLIER DETECTION\n",
    "        X_train_processed = Outlier_Removal(train, \n",
    "                                            OD_majority=OD_majority,\n",
    "                                            OD_minority=OD_minority,\n",
    "                                            )\n",
    "        X_train_processed = apply_smotenc_oversampling(X_train_processed)\n",
    "        # #* OVERSAMPLING & SYNTHETIC DATA GENERATION\n",
    "        print(\"Before oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "        X_train_processed = Synthetic_Data_Generator2(X_train_processed, fold, synthesizer=synthesizer, epochs=epochs, batch_size=256, n_synthetic_data=n_synthetic_data)\n",
    "        \n",
    "        \n",
    "        print(\"After oversampling & synthetic data:\", X_train_processed[[\"DR\"]].value_counts())\n",
    "\n",
    "        \n",
    "        #* Calculate BMI, TCTG & ENCODING\n",
    "        X_train_processed, test = get_bmi(X_train_processed, test)\n",
    "        X_train_processed, test = get_TCTG(X_train_processed, test)\n",
    "        X_train_processed, test = apply_one_hot_encoding(X_train_processed, test)\n",
    "        #* Scaler\n",
    "        X_train_processed[cont_cols] = scaler.fit_transform(X_train_processed[cont_cols])\n",
    "        test[cont_cols] = scaler.transform(test[cont_cols])\n",
    "        # Append processed data (excluding the target column 'DR')\n",
    "\n",
    "        # Save to CSV with fold number\n",
    "        X_train_processed.to_csv(f\"./DATA/folds/train_fold_{fold}.csv\", index=False)\n",
    "        test.to_csv(f\"./DATA/folds/test_fold_{fold}.csv\", index=False)\n",
    "        kFolds_list.append((\n",
    "                            X_train_processed.drop(columns=['DR']),\n",
    "                            test.drop(columns=['DR']),\n",
    "                            X_train_processed['DR'].values.reshape(-1, 1),  # Ensures the target is 2D\n",
    "                            test['DR'].values.reshape(-1, 1)  # Ensures the target is 2D\n",
    "                        ))\n",
    "        break\n",
    "    print(f\"Fold: {fold+1}, Train: {X_train_processed.drop(columns=['DR']).shape}, Test: {test.drop(columns=['DR']).shape}\")\n",
    "    \n",
    "    return kFolds_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68320b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: DR\n",
      "0.0    4129\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "After OD, majority: 2136\n",
      "After OD, minority: 302\n",
      "\n",
      "Applying SMOTENC oversampling...\n",
      "               Age       Gender    Community         UAlb           Ucr  \\\n",
      "count  4272.000000  4272.000000  4272.000000  4272.000000   4272.000000   \n",
      "mean     63.253670     0.556180     3.962313    14.655581   3218.797834   \n",
      "std       6.570588     0.496892     3.128517    18.341817   4891.149370   \n",
      "min      43.000000     0.000000     0.000000     0.100000      1.000000   \n",
      "25%      59.000000     0.000000     1.000000     4.256106      6.000000   \n",
      "50%      63.645130     1.000000     4.000000     8.742674     10.010448   \n",
      "75%      68.000000     1.000000     7.000000    17.400000   6098.374439   \n",
      "max      83.000000     1.000000     9.000000   132.700000  19307.000000   \n",
      "\n",
      "              UACR           TC           TG         LDLC         HDLC  \\\n",
      "count  4272.000000  4272.000000  4272.000000  4272.000000  4272.000000   \n",
      "mean     16.577297     5.181192     1.375707     3.217278     1.351529   \n",
      "std      19.554886     0.835773     0.639705     0.759000     0.245361   \n",
      "min       0.100000     2.640000     0.250000     1.190000     0.730000   \n",
      "25%       5.300000     4.600000     0.900000     2.699240     1.170000   \n",
      "50%       9.798780     5.180000     1.224029     3.220000     1.330000   \n",
      "75%      20.529300     5.740069     1.747804     3.726650     1.518361   \n",
      "max     134.100000     8.150000     3.570000     5.670000     2.210000   \n",
      "\n",
      "               Scr          BUN          FPG        HbA1c       Height  \\\n",
      "count  4272.000000  4272.000000  4272.000000  4272.000000  4272.000000   \n",
      "mean     58.314282     5.773554     8.981263     7.299087   161.477252   \n",
      "std      12.515745     1.204746     2.209823     1.194905     7.008713   \n",
      "min      29.000000     2.600000     4.400000     4.400000   139.000000   \n",
      "25%      49.000000     4.900000     7.400000     6.400000   156.011205   \n",
      "50%      57.000000     5.659655     8.616514     7.116344   160.912269   \n",
      "75%      66.057702     6.600000    10.200000     8.100000   167.000000   \n",
      "max     101.000000    10.000000    18.000000    12.200000   180.000000   \n",
      "\n",
      "            Weight     Duration  \n",
      "count  4272.000000  4272.000000  \n",
      "mean     63.022038     8.441886  \n",
      "std       8.443277     5.497556  \n",
      "min      37.000000     0.100000  \n",
      "25%      56.433630     3.800000  \n",
      "50%      62.293303     7.684615  \n",
      "75%      69.219438    12.762693  \n",
      "max      92.000000    28.000000  \n",
      "\n",
      "Final class distribution: DR\n",
      "0.0    2136\n",
      "1.0    2136\n",
      "Name: count, dtype: int64\n",
      "Before oversampling & synthetic data: DR \n",
      "0.0    2136\n",
      "1.0    2136\n",
      "Name: count, dtype: int64\n",
      "Fitting synthesizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL2\\.venv\\Lib\\site-packages\\sdv\\single_table\\base.py:105: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Gen. (-3.35) | Discrim. (-0.09): 100%|██████████| 500/500 [04:05<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic samples per class based on distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|██████████| 5000/5000 [00:00<00:00, 18668.28it/s]\n",
      "Sampling conditions: 100%|██████████| 5000/5000 [00:00<00:00, 15108.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final synthetic class distribution:\n",
      "DR\n",
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: count, dtype: int64\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 18/18 [00:01<00:00, 15.47it/s]|\n",
      "Column Shapes Score: 89.72%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 153/153 [00:00<00:00, 340.25it/s]|\n",
      "Column Pair Trends Score: 92.39%\n",
      "\n",
      "Overall Score (Average): 91.06%\n",
      "\n",
      "After oversampling & synthetic data: DR \n",
      "0.0    7136\n",
      "1.0    7136\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (14272, 28), Test: (1149, 28)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaler = RobustScaler()\n",
    "scaler = StandardScaler()\n",
    "training_set = pd.read_csv(\"./DATA/training_set/training_data.csv\")\n",
    "\n",
    "kFolds = FOLDS_GENERATOR1(training_set, n_splits=5, random_state=42,             \n",
    "                            OD_majority = IQRDetector(factor=1),\n",
    "                            # OD_minority = IsolationForest(contamination=0.015, random_state=42),\n",
    "                            OD_minority = IQRDetector(factor=1.5),\n",
    "                            synthesizer = \"CTGAN\",\n",
    "                            epochs = 500,\n",
    "                            n_synthetic_data = 10000,\n",
    "                            scaler=scaler,      \n",
    "                            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe1b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa605a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
